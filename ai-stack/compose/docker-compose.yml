# NixOS Hybrid AI Learning Stack - Unified Configuration
# Single source of truth for all AI stack services
# Version: 3.0.0 (Updated December 2025 - Agentic Era Edition)
#
# Services:
#   Core AI Infrastructure:
#   - Qdrant: Vector database for embeddings and context storage
#   - llama.cpp: Local LLM inference (OpenAI-compatible)
#   - Open WebUI: ChatGPT-like interface
#   - PostgreSQL 18 + pgvector: Database for MCP server and metrics
#   - Redis: Caching and session storage
#   - MindsDB: Analytics and data integration
#
#   Agentic Coding Tools (December 2025):
#   - Aider: AI pair programming with git integration
#   - Continue: Open-source autopilot for VS Code and JetBrains
#   - Goose: Autonomous coding agent with file system access
#
#   Video Coding & Content Tools:
#   - LangChain: Agent framework with memory and state management
#   - AutoGPT: Goal decomposition and autonomous planning
#
#   MCP Servers (Model Context Protocol):
#   - AIDB: Context API and telemetry
#   - Hybrid Coordinator: Context augmentation & learning
#   - Ralph Wiggum Loop: Continuous autonomous agent orchestration
#   - NixOS Docs: Centralized Nix/NixOS documentation knowledge base
#
# Ralph Wiggum Loop:
#   - Implements continuous self-referential AI workflow
#   - Enables iterative development until task completion
#   - Default behavior for all agents
#
# Usage:
#   podman-compose up -d
#   ./scripts/hybrid-ai-stack.sh up
#
# Caching & Performance:
#   - All services use pull_policy: missing (only pull if image doesn't exist)
#   - Container images are cached locally and not re-downloaded
#   - GGUF models are cached in ~/.cache/huggingface and ~/.local/share/nixos-ai-stack
#   - Re-running deployment will reuse existing images and models
#   - To force update: podman-compose pull (images) or re-download models manually
#
# Version Policy:
#   - All images pinned to specific versions (no :latest tags)
#   - Prevents unnecessary re-downloads on each deployment
#   - Update versions manually when needed

version: "3.9"

# Disable pod creation for podman-compose (incompatible with network_mode: host)
x-podman:
  in_pod: false
  pod_args:
    - "--userns=keep-id"

services:
  # ============================================================================
  # Core Vector Database
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:v1.16.2  # Latest stable (Dec 2025 - tiered multitenancy + disk-efficient search)
    container_name: local-ai-qdrant
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    network_mode: host  # Use host networking to allow AIDB/coordinator to access via localhost
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/qdrant:/qdrant/storage:Z
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/6333 && echo -e 'GET /healthz HTTP/1.0\\r\\n\\r\\n' >&3 && grep -q '200 OK' <&3"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=qdrant"

  # ============================================================================
  # Sentence Transformers - Dedicated Embedding Service
  # ============================================================================
  # Lightweight embedding service using all-MiniLM-L6-v2 (384D)
  # Provides real semantic embeddings for RAG system
  embeddings:
    build:
      context: ../mcp-servers
      dockerfile: embeddings-service/Dockerfile
    container_name: local-ai-embeddings
    restart: unless-stopped
    network_mode: host
    environment:
      PORT: 8081
      EMBEDDING_MODEL: sentence-transformers/all-MiniLM-L6-v2
      TRANSFORMERS_CACHE: /data/cache
      HF_HOME: /data/cache
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/embeddings:/data:Z
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -sf http://localhost:8081/health | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 180s  # Model download on first run (up to 3 minutes)
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=embeddings"
      - "nixos.quick-deploy.embedding-dimensions=384"

  # ============================================================================
  # llama.cpp - GGUF Model Inference (Primary)
  # ============================================================================
  # Enhanced with Vulkan GPU acceleration for AMD iGPU (December 2025)
  # Inspired by Docker Model Runner innovations
  llama-cpp:
    image: ghcr.io/ggml-org/llama.cpp:server  # Official llama.cpp server (latest)
    container_name: local-ai-llama-cpp
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    command:
      - "--model"
      - "/models/${LLAMA_CPP_MODEL_FILE:-qwen2.5-coder-7b-instruct-q4_k_m.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--ctx-size"
      - "${LLAMA_CTX_SIZE:-8192}"
      - "--threads"
      - "${LLAMA_THREADS:-0}"
      - "--batch-size"
      - "${LLAMA_BATCH_SIZE:-512}"
      - "--ubatch-size"
      - "${LLAMA_UBATCH_SIZE:-128}"
      - "--defrag-thold"
      - "${LLAMA_DEFRAG_THOLD:-0.1}"
      - "--cont-batching"
      - "--parallel"
      - "${LLAMA_PARALLEL:-4}"
      - "--cache-type-k"
      - "${LLAMA_CACHE_TYPE_K:-q4_0}"
      - "--cache-type-v"
      - "${LLAMA_CACHE_TYPE_V:-q4_0}"
      - "--metrics"
      - "--embeddings"
    environment:
      LLAMA_CPP_HOST: 0.0.0.0
      LLAMA_CPP_PORT: 8080
      LLAMA_CPP_DEFAULT_MODEL: ${LLAMA_CPP_DEFAULT_MODEL:-Qwen/Qwen2.5-Coder-7B-Instruct}
      LLAMA_CPP_LOG_LEVEL: ${LLAMA_CPP_LOG_LEVEL:-info}
      LLAMA_CPP_WEB_CONCURRENCY: ${LLAMA_CPP_WEB_CONCURRENCY:-4}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
      # Vulkan GPU acceleration settings (AMD iGPU)
      VK_ICD_FILENAMES: ${VK_ICD_FILENAMES:-/usr/share/vulkan/icd.d/radeon_icd.x86_64.json}
      LLAMA_VULKAN_ENABLE: ${LLAMA_VULKAN_ENABLE:-}  # Set to "1" to enable Vulkan
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/llama-cpp-models:/models:Z
    devices:
      - /dev/dri:/dev/dri  # GPU device access for Vulkan
    network_mode: host  # Use host networking for direct localhost access
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -sf http://localhost:8080/health | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 4G
        limits:
          cpus: '4.0'
          memory: 16G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=llama_cpp"
      - "nixos.quick-deploy.vulkan-enabled=true"
    # GPU support: Vulkan acceleration for AMD/Intel/NVIDIA
    # Set LLAMA_VULKAN_ENABLE=1 to activate
    # Legacy NVIDIA CUDA support (commented - use Vulkan instead):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # ============================================================================
  # Open WebUI - Web Interface
  # ============================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main  # Latest stable (rolling release - Dec 2025)
    container_name: local-ai-open-webui
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    network_mode: host  # Use host networking for localhost access
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/open-webui:/app/backend/data:Z
    environment:
      OPENAI_API_BASE_URL: http://localhost:8080/v1  # Use localhost instead of service name
      OPENAI_API_KEY: llama-cpp
      WEBUI_AUTH: "false"
      ENABLE_RAG_WEB_SEARCH: "true"
      PORT: 3001  # Run on port 3001 (instead of default 8080)
      ENABLE_OLLAMA_API: "false"  # Disable Ollama, use OpenAI-compatible API only
    depends_on:
      llama-cpp:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/3001 && echo -e 'GET / HTTP/1.0\\r\\n\\r\\n' >&3 && grep -q '200 OK' <&3"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          cpus: '1.0'
          memory: 2G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=open-webui"

  # ============================================================================
  # PostgreSQL - Database for MCP and Metrics
  # ============================================================================
  postgres:
    image: pgvector/pgvector:0.8.1-pg18  # PostgreSQL 18 + pgvector 0.8.1
    container_name: local-ai-postgres
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    network_mode: host  # Use host networking to allow AIDB/coordinator to access via localhost
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-mcp}
      POSTGRES_USER: ${POSTGRES_USER:-mcp}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/postgres:/var/lib/postgresql/data:Z
      - ../postgres/init-schema.sql:/docker-entrypoint-initdb.d/01-init-schema.sql:Z
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mcp}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 512M
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=postgres"

  # ============================================================================
  # Redis - Caching and Session Storage
  # ============================================================================
  redis:
    image: redis:8.4.0-alpine  # Latest stable (Dec 2025)
    container_name: local-ai-redis
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    network_mode: host  # Use host networking to allow AIDB/coordinator to access via localhost
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/redis:/data:Z
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    deploy:
      resources:
        reservations:
          cpus: '0.25'
          memory: 256M
        limits:
          cpus: '1.0'
          memory: 1G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=redis"

  # ============================================================================
  # AIDB MCP Server - Context + Telemetry API
  # ============================================================================
  aidb:
    build:
      context: ../mcp-servers
      dockerfile: aidb/Dockerfile
    container_name: local-ai-aidb
    restart: unless-stopped
    network_mode: host
    environment:
      AIDB_CONFIG: /app/config/config.yaml
      # Postgres connection (both AIDB_ and standard POSTGRES_ prefixes for compatibility)
      # Using localhost since network_mode: host allows direct access to host ports
      AIDB_POSTGRES_HOST: localhost
      AIDB_POSTGRES_PORT: 5432
      AIDB_POSTGRES_DB: ${POSTGRES_DB:-mcp}
      AIDB_POSTGRES_USER: ${POSTGRES_USER:-mcp}
      AIDB_POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      POSTGRES_HOST: localhost
      POSTGRES_PORT: 5432
      POSTGRES_DB: ${POSTGRES_DB:-mcp}
      POSTGRES_USER: ${POSTGRES_USER:-mcp}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      # Redis connection
      AIDB_REDIS_HOST: localhost
      AIDB_REDIS_PORT: 6379
      AIDB_REDIS_PASSWORD: ${AIDB_REDIS_PASSWORD:-}
      # Qdrant connection
      QDRANT_URL: http://localhost:6333
      QDRANT_HOST: localhost
      QDRANT_PORT: 6333
      # llama.cpp connection
      LLAMA_CPP_BASE_URL: http://localhost:8080
      # Embeddings service connection
      EMBEDDING_SERVICE_URL: http://localhost:8081
      EMBEDDING_MODEL: sentence-transformers/all-MiniLM-L6-v2
      EMBEDDING_DIMENSIONS: 384
      # Telemetry and discovery
      AIDB_TELEMETRY_PATH: /data/telemetry/aidb-events.jsonl
      AIDB_TOOL_DISCOVERY_ENABLED: ${AIDB_TOOL_DISCOVERY_ENABLED:-true}
      AIDB_TOOL_DISCOVERY_INTERVAL: ${AIDB_TOOL_DISCOVERY_INTERVAL:-300}
      # External integrations
      GOOGLE_SEARCH_API_KEY: ${GOOGLE_SEARCH_API_KEY:-}
      GOOGLE_SEARCH_CX: ${GOOGLE_SEARCH_CX:-}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/aidb:/data:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/telemetry:/data/telemetry:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/aidb-cache:/app/.mcp_cache:Z
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -sf http://localhost:8091/health | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=aidb"

  # ============================================================================
  # MindsDB - Analytics (enabled by default)
  # ============================================================================
  mindsdb:
    image: mindsdb/mindsdb:latest  # Latest stable (rolling release - Dec 2025)
    container_name: local-ai-mindsdb
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    network_mode: host  # Use host networking to allow AIDB/coordinator to access via localhost
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/mindsdb:/root/mindsdb_storage:Z
    environment:
      MINDSDB_STORAGE_DIR: /root/mindsdb_storage
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 3<>/dev/tcp/localhost/47334 && echo -e 'GET / HTTP/1.0\\r\\n\\r\\n' >&3 && grep -q '200 OK' <&3"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 90s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=mindsdb"

  # ============================================================================
  # Hybrid Coordinator MCP Server - Context Augmentation & Learning
  # ============================================================================
  hybrid-coordinator:
    build:
      context: ../mcp-servers
      dockerfile: hybrid-coordinator/Dockerfile
    container_name: local-ai-hybrid-coordinator
    restart: unless-stopped
    network_mode: host
    environment:
      MCP_SERVER_MODE: http
      MCP_SERVER_PORT: 8092
      # Qdrant connection (using localhost since network_mode: host)
      QDRANT_URL: http://localhost:6333
      QDRANT_HOST: localhost
      QDRANT_PORT: 6333
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}
      # Postgres connection (for telemetry storage)
      POSTGRES_HOST: localhost
      POSTGRES_PORT: 5432
      POSTGRES_DB: ${POSTGRES_DB:-mcp}
      POSTGRES_USER: ${POSTGRES_USER:-mcp}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      # llama.cpp connection
      LLAMA_CPP_BASE_URL: http://localhost:8080
      # Embeddings service connection
      EMBEDDING_SERVICE_URL: http://localhost:8081
      EMBEDDING_MODEL: sentence-transformers/all-MiniLM-L6-v2
      EMBEDDING_DIMENSIONS: 384
      # Redis connection (for session management and caching)
      REDIS_URL: redis://localhost:6379
      # Telemetry and learning
      HYBRID_TELEMETRY_PATH: /data/telemetry/hybrid-events.jsonl
      TELEMETRY_PATH: /data/telemetry/hybrid-events.jsonl
      # Continuous learning configuration
      CONTINUOUS_LEARNING_ENABLED: ${CONTINUOUS_LEARNING_ENABLED:-true}
      LEARNING_PROCESSING_INTERVAL: ${LEARNING_PROCESSING_INTERVAL:-3600}
      LEARNING_DATASET_THRESHOLD: ${LEARNING_DATASET_THRESHOLD:-1000}
      # Hybrid mode settings
      LOCAL_CONFIDENCE_THRESHOLD: ${LOCAL_CONFIDENCE_THRESHOLD:-0.7}
      HIGH_VALUE_THRESHOLD: ${HIGH_VALUE_THRESHOLD:-0.7}
      PATTERN_EXTRACTION_ENABLED: ${PATTERN_EXTRACTION_ENABLED:-true}
      FINETUNE_DATA_PATH: /data/fine-tuning/dataset.jsonl
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/hybrid-coordinator:/data:Z,U
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/telemetry:/data/telemetry:Z,U
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/fine-tuning:/data/fine-tuning:Z,U
    depends_on:
      qdrant:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      embeddings:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -sf http://localhost:8092/health | grep -q '\"status\"' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 512M
        limits:
          cpus: '2.0'
          memory: 2G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=hybrid-coordinator"

  # ============================================================================
  # NixOS Documentation MCP Server - Centralized Nix/NixOS Knowledge Base
  # ============================================================================
  nixos-docs:
    build:
      context: ../mcp-servers
      dockerfile: nixos-docs/Dockerfile
    container_name: local-ai-nixos-docs
    restart: unless-stopped
    network_mode: host
    environment:
      # Redis connection
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      # Cache configuration
      NIXOS_CACHE_DIR: /data/cache
      NIXOS_REPOS_DIR: /data/repos
      NIXOS_CACHE_TTL: ${NIXOS_CACHE_TTL:-86400}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/nixos-docs:/data:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/nixos-repos:/data/repos:Z
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8094/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 512M
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=nixos-docs"

  # ============================================================================
  # Aider - AI Pair Programming with Git Integration (Optional - use profile)
  # ============================================================================
  aider:
    image: paulgauthier/aider:latest  # Latest stable (Dec 2025)
    container_name: local-ai-aider
    pull_policy: missing
    restart: unless-stopped
    network_mode: host
    profiles: ["agents", "full"]  # Optional service
    environment:
      OPENAI_API_BASE_URL: http://localhost:8080/v1
      OPENAI_API_KEY: llama-cpp
      AIDER_MODEL: ${AIDER_MODEL:-qwen2.5-coder-7b-instruct}
      AIDER_AUTO_COMMITS: ${AIDER_AUTO_COMMITS:-true}
      AIDER_GIT_DRY_RUN: ${AIDER_GIT_DRY_RUN:-false}
      AIDER_PORT: 8093
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/aider:/data:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/workspace:/workspace:Z
    working_dir: /workspace
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 512M
        limits:
          cpus: '2.0'
          memory: 2G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=aider"

  # ============================================================================
  # NOTE: Continue, Goose, and LangChain services removed (unused)
  # VSCode Continue extension uses llama.cpp directly at port 8080
  # Ralph Wiggum uses aider+autogpt as backends (sufficient for autonomous coding)
  # ============================================================================

  # ============================================================================
  # AutoGPT - Autonomous Goal Decomposition Agent (Optional - use profile)
  # ============================================================================
  autogpt:
    image: significantgravitas/auto-gpt:latest  # Latest stable (Dec 2025)
    container_name: local-ai-autogpt
    pull_policy: missing
    restart: unless-stopped
    network_mode: host
    profiles: ["agents", "full"]  # Optional service
    environment:
      OPENAI_API_BASE: http://localhost:8080/v1
      OPENAI_API_KEY: llama-cpp
      AUTOGPT_MODEL: ${AUTOGPT_MODEL:-qwen2.5-coder-7b-instruct}
      AUTOGPT_PORT: 8097
      MEMORY_BACKEND: ${AUTOGPT_MEMORY_BACKEND:-redis}
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      WORKSPACE_PATH: /workspace
      EXECUTE_LOCAL_COMMANDS: ${AUTOGPT_EXECUTE_COMMANDS:-true}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/autogpt:/data:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/workspace:/workspace:Z
    working_dir: /workspace
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=autogpt"

  # ============================================================================
  # Health Monitor - Self-Healing Infrastructure
  # ============================================================================
  health-monitor:
    build:
      context: ../mcp-servers
      dockerfile: health-monitor/Dockerfile
    container_name: local-ai-health-monitor
    restart: unless-stopped
    network_mode: host
    environment:
      SELF_HEALING_ENABLED: ${SELF_HEALING_ENABLED:-true}
      SELF_HEALING_CHECK_INTERVAL: ${SELF_HEALING_CHECK_INTERVAL:-30}
      SELF_HEALING_COOLDOWN: ${SELF_HEALING_COOLDOWN:-60}
      QDRANT_URL: http://localhost:6333
      POSTGRES_HOST: localhost
      POSTGRES_PORT: 5432
      POSTGRES_DB: ${POSTGRES_DB:-mcp}
      POSTGRES_USER: ${POSTGRES_USER:-mcp}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/health-monitor:/data:Z
      - /var/run/podman/podman.sock:/var/run/docker.sock:Z  # For container management
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      aidb:
        condition: service_healthy
      hybrid-coordinator:
        condition: service_healthy
    privileged: true  # Required for podman container management
    healthcheck:
      test: ["CMD", "python3", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        reservations:
          cpus: '0.25'
          memory: 256M
        limits:
          cpus: '1.0'
          memory: 1G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=health-monitor"
      - "nixos.quick-deploy.self-healing=enabled"

  # ============================================================================
  # Ralph Wiggum Loop - Continuous Autonomous Agent Orchestrator
  # ============================================================================
  aider-wrapper:
    build:
      context: ../mcp-servers
      dockerfile: aider-wrapper/Dockerfile
    container_name: local-ai-aider-wrapper
    restart: unless-stopped
    network_mode: host
    environment:
      AIDER_WRAPPER_PORT: 8099
      PYTHONUNBUFFERED: 1
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/workspace:/workspace:Z
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 512M
        limits:
          cpus: '2.0'
          memory: 2G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=aider-wrapper"
      - "nixos.quick-deploy.agent-backend=aider"

  ralph-wiggum:
    build:
      context: ../mcp-servers
      dockerfile: ralph-wiggum/Dockerfile
    container_name: local-ai-ralph-wiggum
    restart: unless-stopped
    network_mode: host
    # profiles: ["agents", "full"]  # Disabled - now starts by default for agentic workflow
    environment:
      RALPH_MCP_SERVER_PORT: 8098
      RALPH_LOOP_ENABLED: ${RALPH_LOOP_ENABLED:-true}
      RALPH_EXIT_CODE_BLOCK: ${RALPH_EXIT_CODE_BLOCK:-2}
      RALPH_MAX_ITERATIONS: ${RALPH_MAX_ITERATIONS:-0}  # 0 = infinite
      RALPH_CONTEXT_RECOVERY: ${RALPH_CONTEXT_RECOVERY:-true}
      RALPH_GIT_INTEGRATION: ${RALPH_GIT_INTEGRATION:-true}
      RALPH_STATE_FILE: /data/ralph-state.json
      RALPH_TELEMETRY_PATH: /data/telemetry/ralph-events.jsonl
      RALPH_POSTGRES_HOST: localhost
      RALPH_POSTGRES_PORT: 5432
      RALPH_POSTGRES_DB: ${POSTGRES_DB:-mcp}
      RALPH_POSTGRES_USER: ${POSTGRES_USER:-mcp}
      RALPH_POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      RALPH_REDIS_HOST: localhost
      RALPH_REDIS_PORT: 6379
      RALPH_AGENT_BACKENDS: aider,autogpt
      RALPH_DEFAULT_BACKEND: ${RALPH_DEFAULT_BACKEND:-aider}
      RALPH_COORDINATOR_URL: http://localhost:8092
      RALPH_AIDB_URL: http://localhost:8091
      # Human-in-the-Loop controls
      RALPH_REQUIRE_APPROVAL: ${RALPH_REQUIRE_APPROVAL:-false}
      RALPH_APPROVAL_THRESHOLD: ${RALPH_APPROVAL_THRESHOLD:-high}  # low, medium, high
      RALPH_AUDIT_LOG: ${RALPH_AUDIT_LOG:-true}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/ralph-wiggum:/data:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/telemetry:/data/telemetry:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/workspace:/workspace:Z
      - /var/run/podman/podman.sock:/var/run/docker.sock:Z  # For container orchestration
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      aidb:
        condition: service_healthy
      hybrid-coordinator:
        condition: service_healthy
      aider-wrapper:
        condition: service_started
    working_dir: /workspace
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 1G
        limits:
          cpus: '4.0'
          memory: 8G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=ralph-wiggum"
      - "nixos.quick-deploy.ralph-loop=orchestrator"

networks:
  default:
    name: local-ai

volumes:
  # Named volumes managed by compose
  qdrant-data:
  llama-cpp-models:
  open-webui-data:
  postgres-data:
  redis-data:
  mindsdb-data:
