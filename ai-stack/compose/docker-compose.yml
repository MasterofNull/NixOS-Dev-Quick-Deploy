# NixOS Hybrid AI Learning Stack - Unified Configuration
# Single source of truth for all AI stack services
# Version: 2.1.0 (Updated to December 2025 latest stable versions)
#
# Services:
#   - Qdrant: Vector database for embeddings and context storage
#   - Ollama: Local LLM and embeddings (nomic-embed-text)
#   - Lemonade: GGUF model inference (single instance)
#   - Open WebUI: ChatGPT-like interface
#   - PostgreSQL: Database for MCP server and metrics
#   - Redis: Caching and session storage
#
# Usage:
#   podman-compose up -d
#   ./scripts/hybrid-ai-stack.sh up
#
# Version Policy:
#   - All images pinned to specific versions (no :latest tags)
#   - Prevents unnecessary re-downloads on each deployment
#   - Update versions manually when needed

version: "3.9"

services:
  # ============================================================================
  # Core Vector Database
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:v1.16.2  # Latest stable (Dec 2025 - tiered multitenancy + disk-efficient search)
    container_name: local-ai-qdrant
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/qdrant:/qdrant/storage:Z
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:6333/healthz || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=qdrant"

  # ============================================================================
  # Ollama - Local LLM and Embeddings
  # ============================================================================
  ollama:
    image: ollama/ollama:latest  # Latest stable
    container_name: local-ai-ollama
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/ollama:/root/.ollama:Z
    environment:
      OLLAMA_MODELS: /root/.ollama/models
    healthcheck:
      test: ["CMD-SHELL", "ollama list > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 2G
        limits:
          cpus: '4.0'
          memory: 8G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=ollama"
    # GPU support disabled - using CPU (AMD iGPU not supported for inference)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # ============================================================================
  # Lemonade - GGUF Model Inference (Primary)
  # ============================================================================
  lemonade:
    image: ghcr.io/ggml-org/llama.cpp:server  # Official llama.cpp server (latest)
    container_name: local-ai-lemonade
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    command: >
      --model /root/.cache/huggingface/models--Qwen--Qwen2.5-Coder-7B-Instruct-GGUF/snapshots/13fb94bfda8c8cf22497dc57b78f391a9acb426a/qwen2.5-coder-7b-instruct-q4_k_m.gguf
      --host 0.0.0.0
      --port 8080
      --ctx-size 4096
      --n-gpu-layers 0
      --threads 4
    environment:
      LEMONADE_HOST: 0.0.0.0
      LEMONADE_PORT: 8080
      LEMONADE_DEFAULT_MODEL: ${LEMONADE_DEFAULT_MODEL:-Qwen/Qwen2.5-Coder-7B-Instruct}
      LEMONADE_LOG_LEVEL: ${LEMONADE_LOG_LEVEL:-info}
      LEMONADE_WEB_CONCURRENCY: ${LEMONADE_WEB_CONCURRENCY:-4}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:Z
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 4G
        limits:
          cpus: '4.0'
          memory: 16G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=lemonade"
    # GPU support disabled - using CPU (AMD iGPU not supported for inference)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # ============================================================================
  # Open WebUI - Web Interface
  # ============================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main  # Latest stable (rolling release - Dec 2025)
    container_name: local-ai-open-webui
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    ports:
      - "3001:8080"  # Changed from 3000 (conflict with Gitea)
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/open-webui:/app/backend/data:Z
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      WEBUI_AUTH: "false"
      ENABLE_RAG_WEB_SEARCH: "true"
      ENABLE_OLLAMA_API: "true"
    depends_on:
      - ollama
      - lemonade
    deploy:
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          cpus: '1.0'
          memory: 2G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=open-webui"

  # ============================================================================
  # PostgreSQL - Database for MCP and Metrics
  # ============================================================================
  postgres:
    image: pgvector/pgvector:0.8.1-pg18  # Latest stable (PostgreSQL 18 + pgvector 0.8.1 - Dec 2025)
    container_name: local-ai-postgres
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-mcp}
      POSTGRES_USER: ${POSTGRES_USER:-mcp}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/postgres:/var/lib/postgresql/data:Z
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mcp}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 512M
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=postgres"

  # ============================================================================
  # Redis - Caching and Session Storage
  # ============================================================================
  redis:
    image: redis:8.4.0-alpine  # Latest stable (Dec 2025)
    container_name: local-ai-redis
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/redis:/data:Z
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    deploy:
      resources:
        reservations:
          cpus: '0.25'
          memory: 256M
        limits:
          cpus: '1.0'
          memory: 1G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=redis"

  # ============================================================================
  # AIDB MCP Server - Context + Telemetry API
  # ============================================================================
  aidb:
    build:
      context: ../mcp-servers
      dockerfile: aidb/Dockerfile
    container_name: local-ai-aidb
    restart: unless-stopped
    ports:
      - "8091:8091"
    environment:
      AIDB_CONFIG: /app/config/config.yaml
      AIDB_POSTGRES_HOST: postgres
      AIDB_POSTGRES_PORT: 5432
      AIDB_POSTGRES_DB: ${POSTGRES_DB:-mcp}
      AIDB_POSTGRES_USER: ${POSTGRES_USER:-mcp}
      AIDB_POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      AIDB_REDIS_HOST: redis
      AIDB_REDIS_PORT: 6379
      AIDB_REDIS_PASSWORD: ${AIDB_REDIS_PASSWORD:-}
      LEMONADE_BASE_URL: http://lemonade:8080
      AIDB_TELEMETRY_PATH: /data/telemetry/aidb-events.jsonl
      GOOGLE_SEARCH_API_KEY: ${GOOGLE_SEARCH_API_KEY:-}
      GOOGLE_SEARCH_CX: ${GOOGLE_SEARCH_CX:-}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/aidb:/data:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/telemetry:/data/telemetry:Z
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/aidb-cache:/app/.mcp_cache:Z
    depends_on:
      - postgres
      - redis
      - qdrant
      - lemonade
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8091/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=aidb"

  # ============================================================================
  # MindsDB - Analytics (enabled by default)
  # ============================================================================
  mindsdb:
    image: mindsdb/mindsdb:latest  # Latest stable (rolling release - Dec 2025)
    container_name: local-ai-mindsdb
    pull_policy: missing  # Only pull if image doesn't exist locally
    restart: unless-stopped
    ports:
      - "47334:47334"
      - "47335:47335"
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/mindsdb:/root/mindsdb_storage:Z
    environment:
      MINDSDB_STORAGE_DIR: /root/mindsdb_storage
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=mindsdb"

  # ============================================================================
  # Hybrid Coordinator MCP Server - Context Augmentation & Learning
  # ============================================================================
  hybrid-coordinator:
    build:
      context: ../mcp-servers
      dockerfile: hybrid-coordinator/Dockerfile
    container_name: local-ai-hybrid-coordinator
    restart: unless-stopped
    ports:
      - "8092:8092"
    environment:
      MCP_SERVER_MODE: http
      MCP_SERVER_PORT: 8092
      QDRANT_URL: http://qdrant:6333
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}
      LEMONADE_BASE_URL: http://lemonade:8080
      OLLAMA_BASE_URL: http://ollama:11434
      HYBRID_TELEMETRY_PATH: /data/telemetry/hybrid-events.jsonl
      LOCAL_CONFIDENCE_THRESHOLD: ${LOCAL_CONFIDENCE_THRESHOLD:-0.7}
      HIGH_VALUE_THRESHOLD: ${HIGH_VALUE_THRESHOLD:-0.7}
      PATTERN_EXTRACTION_ENABLED: ${PATTERN_EXTRACTION_ENABLED:-true}
      FINETUNE_DATA_PATH: /data/fine-tuning/dataset.jsonl
      TELEMETRY_PATH: /data/telemetry/hybrid-events.jsonl
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/hybrid-coordinator:/data:Z,U
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/telemetry:/data/telemetry:Z,U
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/fine-tuning:/data/fine-tuning:Z,U
    depends_on:
      - qdrant
      - ollama
      - lemonade
      - aidb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8092/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          cpus: '0.5'
          memory: 512M
        limits:
          cpus: '2.0'
          memory: 2G
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=hybrid-coordinator"

networks:
  default:
    name: local-ai

volumes:
  # Named volumes managed by compose
  qdrant-data:
  ollama-data:
  lemonade-models:
  open-webui-data:
  postgres-data:
  redis-data:
  mindsdb-data:
