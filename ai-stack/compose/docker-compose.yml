# NixOS Hybrid AI Learning Stack - Unified Configuration
# Single source of truth for all AI stack services
# Version: 2.0.0 (Unified)
#
# Services:
#   - Qdrant: Vector database for embeddings and context storage
#   - Ollama: Local LLM and embeddings (nomic-embed-text)
#   - Lemonade: GGUF model inference (3 instances for different models)
#   - Open WebUI: ChatGPT-like interface
#   - PostgreSQL: Database for MCP server and metrics
#   - Redis: Caching and session storage
#
# Usage:
#   podman-compose up -d
#   ./scripts/hybrid-ai-stack.sh up

version: "3.9"

services:
  # ============================================================================
  # Core Vector Database
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:latest
    container_name: local-ai-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/qdrant:/qdrant/storage:Z
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=qdrant"

  # ============================================================================
  # Ollama - Local LLM and Embeddings
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: local-ai-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/ollama:/root/.ollama:Z
    environment:
      OLLAMA_MODELS: /root/.ollama/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=ollama"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ============================================================================
  # Lemonade - GGUF Model Inference (Primary)
  # ============================================================================
  lemonade:
    image: ghcr.io/json012/lemonade:latest
    container_name: local-ai-lemonade
    restart: unless-stopped
    environment:
      LEMONADE_HOST: 0.0.0.0
      LEMONADE_PORT: 8080
      LEMONADE_DEFAULT_MODEL: ${LEMONADE_DEFAULT_MODEL:-Qwen/Qwen2.5-Coder-7B-Instruct}
      LEMONADE_LOG_LEVEL: ${LEMONADE_LOG_LEVEL:-info}
      LEMONADE_WEB_CONCURRENCY: ${LEMONADE_WEB_CONCURRENCY:-4}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/lemonade-models:/home/ubuntu/.cache/huggingface:Z
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=lemonade"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ============================================================================
  # Open WebUI - Web Interface
  # ============================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: local-ai-open-webui
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/open-webui:/app/backend/data:Z
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      WEBUI_AUTH: "false"
      ENABLE_RAG_WEB_SEARCH: "true"
      ENABLE_OLLAMA_API: "true"
    depends_on:
      - ollama
      - lemonade
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=open-webui"

  # ============================================================================
  # PostgreSQL - Database for MCP and Metrics
  # ============================================================================
  postgres:
    image: pgvector/pgvector:pg16
    container_name: local-ai-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-mcp}
      POSTGRES_USER: ${POSTGRES_USER:-mcp}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me_in_production}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/postgres:/var/lib/postgresql/data:Z
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mcp}"]
      interval: 10s
      timeout: 5s
      retries: 5
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=postgres"

  # ============================================================================
  # Redis - Caching and Session Storage
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: local-ai-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/redis:/data:Z
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=redis"

  # ============================================================================
  # MindsDB - Optional Analytics (if enabled)
  # ============================================================================
  mindsdb:
    image: mindsdb/mindsdb:latest
    container_name: local-ai-mindsdb
    restart: unless-stopped
    ports:
      - "47334:47334"
      - "47335:47335"
    volumes:
      - ${AI_STACK_DATA:-~/.local/share/nixos-ai-stack}/mindsdb:/root/mindsdb_storage:Z
    environment:
      MINDSDB_STORAGE_DIR: /root/mindsdb_storage
    profiles:
      - analytics
    labels:
      - "nixos.quick-deploy.ai-stack=true"
      - "nixos.quick-deploy.service=mindsdb"

networks:
  default:
    name: local-ai

volumes:
  # Named volumes managed by compose
  qdrant-data:
  ollama-data:
  lemonade-models:
  open-webui-data:
  postgres-data:
  redis-data:
  mindsdb-data:
