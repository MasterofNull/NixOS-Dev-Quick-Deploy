# NixOS-Dev-Quick-Deploy AI Stack Environment Configuration
# Version: 6.0.0 (Integrated)
# Copy to ~/.config/nixos-ai-stack/.env and adjust as needed

# ============================================================================
# Shared Data Paths (NEW in v6.0.0)
# ============================================================================
# All data persists in shared directories that survive reinstalls
AI_STACK_DATA=${HOME}/.local/share/nixos-ai-stack
AI_STACK_CONFIG=${HOME}/.config/nixos-ai-stack
AI_STACK_PROFILE=personal  # personal|guest|none

# ============================================================================
# Model Configuration
# ============================================================================
# Default model for llama.cpp inference
LLAMA_CPP_DEFAULT_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct

# Default embeddings model (sentence-transformers)
# Updated January 2026: BGE-small is a 2024/2025 model with better quality than MiniLM
# - 384 dimensions (compatible with existing Qdrant setup)
# - Excellent for code and documentation retrieval
# - MIT licensed, optimized for semantic search
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# HuggingFace token (optional, for private models)
HUGGING_FACE_HUB_TOKEN=

# llama.cpp configuration
LLAMA_CPP_HOST=0.0.0.0
LLAMA_CPP_PORT=8080
LLAMA_CPP_LOG_LEVEL=info
LLAMA_CPP_WEB_CONCURRENCY=4
LLAMA_CPP_CTX_SIZE=4096
LLAMA_CPP_BASE_URL=http://llama-cpp:8080

# GGUF model filename (auto-set by deployment, maps from LLAMA_CPP_DEFAULT_MODEL)
# This is the actual .gguf file that llama.cpp will load
LLAMA_CPP_MODEL_FILE=qwen2.5-coder-7b-instruct-q4_k_m.gguf

# CPU Optimization Parameters
# Context size: Larger = more memory but better understanding (2048-32768)
LLAMA_CTX_SIZE=8192

# Threads: 0 = auto-detect optimal count based on CPU cores
LLAMA_THREADS=0

# Batch processing parameters
LLAMA_BATCH_SIZE=512          # Physical batch size (higher = more throughput, more RAM)
LLAMA_UBATCH_SIZE=128         # Micro-batch for attention (lower = less VRAM/RAM spikes)

# KV cache quantization (reduces RAM by ~50-70%)
# Options: f16, q8_0, q4_0, q4_1, q5_0, q5_1, iq4_nl
LLAMA_CACHE_TYPE_K=q4_0       # Key cache quantization
LLAMA_CACHE_TYPE_V=q4_0       # Value cache quantization

# Memory defragmentation threshold (0.0-1.0, lower = more aggressive)
LLAMA_DEFRAG_THOLD=0.1

# Parallel request processing (1-16, higher = more concurrent requests)
LLAMA_PARALLEL=4

# ============================================================================
# Database Configuration
# ============================================================================
# PostgreSQL (TimescaleDB)
AIDB_POSTGRES_DB=mcp
AIDB_POSTGRES_USER=mcp
# AIDB_POSTGRES_PASSWORD - MOVED TO DOCKER SECRETS (Day 5)
# Password is stored in: ai-stack/compose/secrets/postgres_password
# AIDB reads password from PostgreSQL connection (inherits from POSTGRES_PASSWORD_FILE)
# AIDB_POSTGRES_PASSWORD=<removed for security>
AIDB_POSTGRES_HOST=postgres
AIDB_POSTGRES_PORT=5432

# Redis Password - MOVED TO DOCKER SECRETS (Day 5)
# Password is stored in: ai-stack/compose/secrets/redis_password
# Redis reads from: /run/secrets/redis_password (via --requirepass)
# AIDB_REDIS_PASSWORD=<removed for security>
AIDB_REDIS_HOST=redis
AIDB_REDIS_PORT=6379

# Qdrant Vector Database
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# ============================================================================
# Service Ports (Configurable for Conflict Resolution)
# ============================================================================
# Adjust these if you have port conflicts
AIDB_PORT=8091          # AIDB MCP Server
# LLAMA_CPP_PORT defined above in llama.cpp configuration section
POSTGRES_PORT=5432      # PostgreSQL
REDIS_PORT=6379         # Redis
QDRANT_PORT=6333        # Qdrant
REDIS_INSIGHT_PORT=5540 # Redis Insight web UI

# ============================================================================
# MCP Server Configuration
# ============================================================================
AIDB_CONFIG=/app/config/config.yaml

# ============================================================================
# External Integrations (Optional)
# ============================================================================
# Google Search API (for agent web search)
GOOGLE_SEARCH_API_KEY=
GOOGLE_SEARCH_CX=

# ============================================================================
# Observability (Optional)
# ============================================================================
GRAFANA_ADMIN_USER=admin
# Grafana Admin Password - MOVED TO DOCKER SECRETS (Day 5)
# Password is stored in: ai-stack/compose/secrets/grafana_admin_password
# Grafana reads from: /run/secrets/grafana_admin_password (via GF_SECURITY_ADMIN_PASSWORD__FILE)
# DO NOT use default password 'admin' in production!
# GRAFANA_ADMIN_PASSWORD=<removed for security>

# Log importer configuration
LOG_IMPORT_PATH=/data/logs/runs.jsonl
LOG_IMPORT_DSN=
LOG_IMPORT_INTERVAL=900
LOG_IMPORT_ROTATE=1
LOG_IMPORT_ROTATE_KEEP=5

# ============================================================================
# Development Overrides
# ============================================================================
# Uncomment for development mode
# DEBUG=true
# LOG_LEVEL=debug
# RELOAD=true  # Auto-reload on code changes

# ============================================================================
# Notes
# ============================================================================
# 1. This file is a template. Copy it to ~/.config/nixos-ai-stack/.env
# 2. Update passwords and API keys before deploying
# 3. AI_STACK_DATA path is where all persistent data lives
# 4. See docs/AI-STACK-FULL-INTEGRATION.md for detailed configuration
# 5. Model registry: ai-stack/models/registry.json

# ============================================================================
# Hybrid Learning Configuration
# ============================================================================
HYBRID_MODE_ENABLED=true
LOCAL_CONFIDENCE_THRESHOLD=0.7
HIGH_VALUE_THRESHOLD=0.7
# RE-ENABLED: Uses local llama.cpp only, helps improve filtering over time
# Does not affect remote token usage
PATTERN_EXTRACTION_ENABLED=true
AUTO_FINETUNE_ENABLED=false

# Paths
FINETUNE_DATA_PATH=~/.local/share/nixos-ai-stack/fine-tuning/dataset.jsonl

# ============================================================================
# Vibe Coding Features Configuration (v3.0.0 - Agentic Era)
# ============================================================================

# Tool Discovery Engine
AIDB_TOOL_DISCOVERY_ENABLED=true
AIDB_TOOL_DISCOVERY_INTERVAL=300  # Scan MCP servers every 5 minutes

# Self-Healing Infrastructure
SELF_HEALING_ENABLED=true
SELF_HEALING_CHECK_INTERVAL=30    # Check container health every 30 seconds
SELF_HEALING_COOLDOWN=60          # Wait 60 seconds between restart attempts

# Continuous Learning Pipeline
# RE-ENABLED: Does not affect remote token usage (runs in background)
# Actually helps improve local filtering over time
CONTINUOUS_LEARNING_ENABLED=true
LEARNING_PROCESSING_INTERVAL=3600  # Process telemetry every hour
LEARNING_DATASET_THRESHOLD=1000    # Export dataset when 1000+ examples

# ===========================================================================
# REMOTE TOKEN OPTIMIZATION (Day 1)
# These settings reduce context sent to remote agents (Claude Code, etc.)
# ===========================================================================

# Query Expansion (MAJOR TOKEN WASTER)
# Expands 1 query into 3-5 queries, returns 3-5x more context
# Remote agent has to process ALL of it with paid API
# DISABLED: Saves 60-70% of remote tokens
QUERY_EXPANSION_ENABLED=false
QUERY_EXPANSION_MAX_EXPANSIONS=2  # If re-enabled, limit to 2 not 5

# Remote LLM Feedback (CREATES EXTRA ROUND-TRIPS)
# Allows remote agents to request more context iteratively
# Each round-trip = additional Anthropic API call
# DISABLED: Prevents 1-2 extra API calls per query
REMOTE_LLM_FEEDBACK_ENABLED=false

# Multi-Turn Context Query Expansion
# Even within sessions, don't expand queries (wastes tokens)
MULTI_TURN_QUERY_EXPANSION=false

# Default Token Budgets (REDUCED)
# Lower defaults save tokens when remote agent doesn't specify
DEFAULT_MAX_TOKENS=1000  # Down from 2000
PROGRESSIVE_DISCLOSURE_OVERVIEW_MAX=200  # Overview level
PROGRESSIVE_DISCLOSURE_DETAILED_MAX=600  # Detailed level
PROGRESSIVE_DISCLOSURE_COMPREHENSIVE_MAX=1500  # Comprehensive level

# Context Compression (KEEP - This is good!)
CONTEXT_COMPRESSION_ENABLED=true
CONTEXT_COMPRESSION_STRATEGY=hybrid  # truncate|summarize|hybrid

# Semantic Caching (EXPANDED - Reduces redundant processing)
REDIS_CACHE_TTL_SECONDS=86400  # 24 hours for stable content
SEMANTIC_CACHE_SIMILARITY_THRESHOLD=0.95

# Ralph Wiggum Loop Engine
RALPH_LOOP_ENABLED=true
# Exit code that triggers loop continuation
RALPH_EXIT_CODE_BLOCK=2
# 0 = infinite loop until task complete
RALPH_MAX_ITERATIONS=0
# Recover context from git on restart
RALPH_CONTEXT_RECOVERY=true
# Enable git operations
RALPH_GIT_INTEGRATION=true
# Human-in-the-loop approval for high-risk actions
RALPH_REQUIRE_APPROVAL=false
# low, medium, high
RALPH_APPROVAL_THRESHOLD=high
# Log all Ralph actions
RALPH_AUDIT_LOG=true
# Default coding agent (aider, continue, goose, etc.)
RALPH_DEFAULT_BACKEND=aider

# PostgreSQL Password - MOVED TO DOCKER SECRETS (Day 5)
# Password is stored in: ai-stack/compose/secrets/postgres_password
# PostgreSQL reads from: /run/secrets/postgres_password (via POSTGRES_PASSWORD_FILE)
# DO NOT store plaintext passwords in this file!
# POSTGRES_PASSWORD=<removed for security>

# ============================================================================
# Secure Container Management (Day 2 - Week 1)
# ============================================================================
# Podman REST API - replaces privileged containers and socket mounts
# Services use HTTP API instead of direct socket access
# NOTE: Using actual host IP instead of host.containers.internal due to podman networking
# If host IP changes, update this value (check with: ip addr show | grep "inet ")
PODMAN_API_URL=http://192.168.86.145:2375
PODMAN_API_VERSION=v4.0.0

# Container operation audit logging
CONTAINER_AUDIT_ENABLED=true
CONTAINER_AUDIT_LOG_PATH=/data/telemetry/container-audit.jsonl

# Operation allowlists (comma-separated)
HEALTH_MONITOR_ALLOWED_OPS=list,inspect,restart
RALPH_WIGGUM_ALLOWED_OPS=list,inspect,create,start,stop,logs
CONTAINER_ENGINE_ALLOWED_OPS=list,inspect,logs
