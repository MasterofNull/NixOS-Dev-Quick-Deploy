# NixOS-Dev-Quick-Deploy AI Stack Environment Configuration
# Version: 6.0.0 (Integrated)
# Copy to ~/.config/nixos-ai-stack/.env and adjust as needed

# ============================================================================
# Shared Data Paths (NEW in v6.0.0)
# ============================================================================
# All data persists in shared directories that survive reinstalls
AI_STACK_DATA=${HOME}/.local/share/nixos-ai-stack
AI_STACK_CONFIG=${HOME}/.config/nixos-ai-stack
AI_STACK_PROFILE=personal  # personal|guest|none

# ============================================================================
# Model Configuration
# ============================================================================
# Default model for llama.cpp inference
LLAMA_CPP_DEFAULT_MODEL=unsloth/Qwen3-4B-Instruct-2507-GGUF

# Default embeddings model (sentence-transformers)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# HuggingFace token (optional, for private models)
HUGGING_FACE_HUB_TOKEN=

# llama.cpp configuration
LLAMA_CPP_HOST=0.0.0.0
LLAMA_CPP_PORT=8080
LLAMA_CPP_LOG_LEVEL=info
LLAMA_CPP_WEB_CONCURRENCY=4
LLAMA_CPP_CTX_SIZE=4096
LLAMA_CPP_BASE_URL=http://llama-cpp:8080

# GGUF model filename (auto-set by deployment, maps from LLAMA_CPP_DEFAULT_MODEL)
# This is the actual .gguf file that llama.cpp will load
LLAMA_CPP_MODEL_FILE=Qwen3-4B-Instruct-2507-Q4_K_M.gguf

# CPU Optimization Parameters
# Context size: Larger = more memory but better understanding (2048-32768)
LLAMA_CTX_SIZE=8192

# Threads: 0 = auto-detect optimal count based on CPU cores
LLAMA_THREADS=0

# Batch processing parameters
LLAMA_BATCH_SIZE=512          # Physical batch size (higher = more throughput, more RAM)
LLAMA_UBATCH_SIZE=128         # Micro-batch for attention (lower = less VRAM/RAM spikes)

# KV cache quantization (reduces RAM by ~50-70%)
# Options: f16, q8_0, q4_0, q4_1, q5_0, q5_1, iq4_nl
LLAMA_CACHE_TYPE_K=q4_0       # Key cache quantization
LLAMA_CACHE_TYPE_V=q4_0       # Value cache quantization

# Memory defragmentation threshold (0.0-1.0, lower = more aggressive)
LLAMA_DEFRAG_THOLD=0.1

# Parallel request processing (1-16, higher = more concurrent requests)
LLAMA_PARALLEL=4

# ============================================================================
# Database Configuration
# ============================================================================
# PostgreSQL (TimescaleDB)
AIDB_POSTGRES_DB=mcp
AIDB_POSTGRES_USER=mcp
AIDB_POSTGRES_PASSWORD=change_me_in_production
AIDB_POSTGRES_HOST=postgres
AIDB_POSTGRES_PORT=5432

# Redis
AIDB_REDIS_PASSWORD=
AIDB_REDIS_HOST=redis
AIDB_REDIS_PORT=6379

# Qdrant Vector Database
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# ============================================================================
# Service Ports (Configurable for Conflict Resolution)
# ============================================================================
# Adjust these if you have port conflicts
AIDB_PORT=8091          # AIDB MCP Server
# LLAMA_CPP_PORT defined above in llama.cpp configuration section
POSTGRES_PORT=5432      # PostgreSQL
REDIS_PORT=6379         # Redis
QDRANT_PORT=6333        # Qdrant
REDIS_INSIGHT_PORT=5540 # Redis Insight web UI

# ============================================================================
# MCP Server Configuration
# ============================================================================
AIDB_CONFIG=/app/config/config.yaml

# ============================================================================
# External Integrations (Optional)
# ============================================================================
# Google Search API (for agent web search)
GOOGLE_SEARCH_API_KEY=
GOOGLE_SEARCH_CX=

# ============================================================================
# Observability (Optional)
# ============================================================================
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# Log importer configuration
LOG_IMPORT_PATH=/data/logs/runs.jsonl
LOG_IMPORT_DSN=
LOG_IMPORT_INTERVAL=900
LOG_IMPORT_ROTATE=1
LOG_IMPORT_ROTATE_KEEP=5

# ============================================================================
# Development Overrides
# ============================================================================
# Uncomment for development mode
# DEBUG=true
# LOG_LEVEL=debug
# RELOAD=true  # Auto-reload on code changes

# ============================================================================
# Notes
# ============================================================================
# 1. This file is a template. Copy it to ~/.config/nixos-ai-stack/.env
# 2. Update passwords and API keys before deploying
# 3. AI_STACK_DATA path is where all persistent data lives
# 4. See docs/AI-STACK-FULL-INTEGRATION.md for detailed configuration
# 5. Model registry: ai-stack/models/registry.json

# ============================================================================
# Hybrid Learning Configuration
# ============================================================================
HYBRID_MODE_ENABLED=true
LOCAL_CONFIDENCE_THRESHOLD=0.7
HIGH_VALUE_THRESHOLD=0.7
PATTERN_EXTRACTION_ENABLED=true
AUTO_FINETUNE_ENABLED=false

# Paths
FINETUNE_DATA_PATH=~/.local/share/nixos-ai-stack/fine-tuning/dataset.jsonl

# ============================================================================
# Vibe Coding Features Configuration (v3.0.0 - Agentic Era)
# ============================================================================

# Tool Discovery Engine
AIDB_TOOL_DISCOVERY_ENABLED=true
AIDB_TOOL_DISCOVERY_INTERVAL=300  # Scan MCP servers every 5 minutes

# Self-Healing Infrastructure
SELF_HEALING_ENABLED=true
SELF_HEALING_CHECK_INTERVAL=30    # Check container health every 30 seconds
SELF_HEALING_COOLDOWN=60          # Wait 60 seconds between restart attempts

# Continuous Learning Pipeline
CONTINUOUS_LEARNING_ENABLED=true
LEARNING_PROCESSING_INTERVAL=3600  # Process telemetry every hour
LEARNING_DATASET_THRESHOLD=1000    # Export dataset when 1000+ examples

# Ralph Wiggum Loop Engine
RALPH_LOOP_ENABLED=true
# Exit code that triggers loop continuation
RALPH_EXIT_CODE_BLOCK=2
# 0 = infinite loop until task complete
RALPH_MAX_ITERATIONS=0
# Recover context from git on restart
RALPH_CONTEXT_RECOVERY=true
# Enable git operations
RALPH_GIT_INTEGRATION=true
# Human-in-the-loop approval for high-risk actions
RALPH_REQUIRE_APPROVAL=false
# low, medium, high
RALPH_APPROVAL_THRESHOLD=high
# Log all Ralph actions
RALPH_AUDIT_LOG=true
# Default coding agent (aider, continue, goose, etc.)
RALPH_DEFAULT_BACKEND=aider
POSTGRES_PASSWORD=change_me_in_production
