#!/usr/bin/env python3
"""
Remote LLM Feedback API
Allows remote LLMs to report confidence and request additional context
Enables recursive refinement in RLM pattern
"""

import asyncio
import logging
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
from uuid import uuid4

import httpx
from pydantic import BaseModel, Field

logger = logging.getLogger("remote-llm-feedback")


class FeedbackRequest(BaseModel):
    """Feedback from remote LLM about response quality"""
    session_id: str = Field(..., description="Session identifier")
    response: str = Field(..., description="The response generated by remote LLM")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score (0.0-1.0)")
    gaps: List[str] = Field(default_factory=list, description="Identified information gaps")
    metadata: Optional[Dict[str, Any]] = Field(default=None)


class FeedbackResponse(BaseModel):
    """System response to feedback"""
    suggested_queries: List[str] = Field(..., description="Suggested queries to fill gaps")
    estimated_confidence_increase: float = Field(..., description="Expected confidence boost")
    should_refine: bool = Field(..., description="Whether refinement is recommended")
    available_collections: List[str] = Field(..., description="Collections that may help")
    session_info: Dict[str, Any] = Field(..., description="Current session state")


class ConfidenceAnalysis(BaseModel):
    """Analysis of confidence level and gaps"""
    confidence_level: str  # low, medium, high
    primary_gaps: List[str]
    recommended_actions: List[str]
    estimated_improvement: float


class RemoteLLMFeedback:
    """
    API for remote LLMs to report response quality and request refinement

    Enables RLM (Recursive Language Model) pattern where remote LLMs can:
    1. Generate initial response
    2. Self-evaluate confidence
    3. Report low confidence to this API
    4. Receive suggestions for additional context queries
    5. Query for specific additional context
    6. Generate refined response

    Example usage by Claude:

    # After generating initial response
    feedback_response = await feedback_api.evaluate_response(
        session_id="uuid",
        response="Here's how to fix the error...",
        confidence=0.65,
        gaps=[
            "Missing specific systemd service configuration",
            "Unclear about dependencies between services"
        ]
    )

    # Response suggests specific queries:
    # ["Get systemd service examples for GNOME", "Show service dependency configuration"]

    # Claude then queries for additional context using those suggestions
    """

    def __init__(
        self,
        qdrant_client,
        multi_turn_manager,
        llama_cpp_url: str = "http://localhost:8080",
        confidence_threshold: float = 0.75
    ):
        self.qdrant = qdrant_client
        self.multi_turn_manager = multi_turn_manager
        self.llama_cpp_url = llama_cpp_url
        self.confidence_threshold = confidence_threshold

        # Confidence level thresholds
        self.confidence_levels = {
            "high": 0.80,
            "medium": 0.60,
            "low": 0.0
        }

    async def evaluate_response(
        self,
        session_id: str,
        response: str,
        confidence: float,
        gaps: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> FeedbackResponse:
        """
        Process remote LLM's self-evaluation and suggest refinement paths

        Args:
            session_id: Session identifier
            response: The response generated by remote LLM
            confidence: Self-reported confidence (0.0-1.0)
            gaps: Identified information gaps
            metadata: Optional metadata

        Returns:
            FeedbackResponse with suggestions and recommendations
        """
        start_time = datetime.now(timezone.utc)

        gaps = gaps or []

        logger.info(
            f"Session {session_id}: Received feedback with confidence {confidence:.2f}, "
            f"{len(gaps)} gaps identified"
        )

        # Analyze confidence level
        analysis = await self.analyze_confidence(confidence, gaps, response)

        # Generate targeted queries to fill gaps
        suggested_queries = []
        for gap in gaps:
            query = await self.gap_to_query(gap)
            if query:
                suggested_queries.append(query)

        # If no specific gaps provided, generate general suggestions
        if not suggested_queries and confidence < self.confidence_threshold:
            suggested_queries = await self.generate_improvement_queries(
                session_id,
                response,
                confidence
            )

        # Estimate how much confidence would increase
        estimated_increase = await self.estimate_confidence_gain(
            gaps=gaps,
            available_context=await self.check_available_context(suggested_queries)
        )

        # Get available collections that might help
        available_collections = await self.identify_helpful_collections(gaps, response)

        # Get session info
        session_info = await self.multi_turn_manager.get_session_info(session_id)
        if not session_info:
            session_info = {"status": "session_not_found"}

        # Record feedback for learning
        await self.record_feedback(
            session_id=session_id,
            confidence=confidence,
            gaps=gaps,
            suggested_queries=suggested_queries,
            metadata=metadata
        )

        duration = (datetime.now(timezone.utc) - start_time).total_seconds()
        logger.info(
            f"Session {session_id}: Generated {len(suggested_queries)} suggestions "
            f"in {duration:.2f}s, estimated +{estimated_increase:.2f} confidence"
        )

        return FeedbackResponse(
            suggested_queries=suggested_queries,
            estimated_confidence_increase=estimated_increase,
            should_refine=(confidence < self.confidence_threshold),
            available_collections=available_collections,
            session_info=session_info
        )

    async def analyze_confidence(
        self,
        confidence: float,
        gaps: List[str],
        response: str
    ) -> ConfidenceAnalysis:
        """Analyze confidence level and provide recommendations"""

        # Determine confidence level
        if confidence >= self.confidence_levels["high"]:
            level = "high"
        elif confidence >= self.confidence_levels["medium"]:
            level = "medium"
        else:
            level = "low"

        # Identify primary gaps (most important first)
        primary_gaps = gaps[:3] if gaps else ["General uncertainty"]

        # Generate recommendations
        recommendations = []
        if level == "low":
            recommendations.append("Request detailed context from multiple collections")
            recommendations.append("Consider breaking down the query into sub-questions")
            recommendations.append("Query for specific examples and configurations")
        elif level == "medium":
            recommendations.append("Request additional context for uncertain areas")
            recommendations.append("Verify key facts against knowledge base")
        else:
            recommendations.append("Response appears confident, consider finalizing")

        # Estimate potential improvement
        # More gaps = more potential improvement if addressed
        estimated_improvement = min(0.3, len(gaps) * 0.1)

        return ConfidenceAnalysis(
            confidence_level=level,
            primary_gaps=primary_gaps,
            recommended_actions=recommendations,
            estimated_improvement=estimated_improvement
        )

    async def gap_to_query(self, gap: str) -> Optional[str]:
        """
        Convert identified gap into specific search query

        Uses local LLM to transform abstract gap into concrete query

        Example:
        Gap: "Missing specific systemd service configuration"
        Query: "NixOS systemd service configuration examples"
        """
        prompt = f"""Convert this information gap into a specific, concrete search query:

Gap: {gap}

Search query (one line, specific and actionable):"""

        try:
            async with httpx.AsyncClient(timeout=15.0) as client:
                response = await client.post(
                    f"{self.llama_cpp_url}/v1/completions",
                    json={
                        "prompt": prompt,
                        "max_tokens": 50,
                        "temperature": 0.5,
                        "stop": ["\n", "Gap:"]
                    }
                )

                if response.status_code == 200:
                    data = response.json()
                    query = data["choices"][0]["text"].strip()
                    # Clean up common prefixes
                    query = query.lstrip("Query: ").lstrip("Search: ").strip()
                    return query if len(query) > 5 else None

        except Exception as e:
            logger.error(f"Error converting gap to query: {e}")

        # Fallback: use gap directly as query
        return gap

    async def generate_improvement_queries(
        self,
        session_id: str,
        response: str,
        confidence: float
    ) -> List[str]:
        """
        Generate general improvement queries when no specific gaps provided

        Analyzes the response and current confidence to suggest
        what additional information would help
        """
        # Get session context
        session_info = await self.multi_turn_manager.get_session_info(session_id)

        if not session_info:
            return []

        # Build prompt for query generation
        prompt = f"""A remote AI generated this response with confidence {confidence:.2f} (target: 0.75+).
Suggest 2-3 specific queries to gather additional information that would improve confidence.

Original query: {session_info.get('queries', ['Unknown'])[-1]}

Response excerpt:
{response[:300]}...

Suggest 2-3 specific search queries (one per line):"""

        try:
            async with httpx.AsyncClient(timeout=20.0) as client:
                llm_response = await client.post(
                    f"{self.llama_cpp_url}/v1/completions",
                    json={
                        "prompt": prompt,
                        "max_tokens": 150,
                        "temperature": 0.7,
                        "stop": ["\n\n"]
                    }
                )

                if llm_response.status_code == 200:
                    data = llm_response.json()
                    text = data["choices"][0]["text"].strip()

                    # Parse queries (one per line)
                    queries = [
                        line.strip().lstrip("123456789.-â€¢ ")
                        for line in text.split("\n")
                        if line.strip() and len(line.strip()) > 10
                    ]

                    return queries[:3]

        except Exception as e:
            logger.error(f"Error generating improvement queries: {e}")

        return []

    async def estimate_confidence_gain(
        self,
        gaps: List[str],
        available_context: Dict[str, int]
    ) -> float:
        """
        Estimate how much confidence would increase if gaps are addressed

        Args:
            gaps: List of identified gaps
            available_context: Dict of collection -> available_results_count

        Returns:
            Estimated confidence increase (0.0-0.30)
        """
        if not gaps:
            return 0.0

        # Base gain per gap
        base_gain_per_gap = 0.08

        # Bonus if we have context available
        context_available = sum(available_context.values()) > 0
        availability_multiplier = 1.5 if context_available else 0.7

        # Calculate total estimated gain
        estimated_gain = len(gaps) * base_gain_per_gap * availability_multiplier

        # Cap at 0.30 (realistic maximum improvement)
        return min(0.30, estimated_gain)

    async def check_available_context(
        self,
        queries: List[str]
    ) -> Dict[str, int]:
        """
        Check which collections have context for the suggested queries

        Returns:
            Dict of collection_name -> count of available results
        """
        available = {}

        collections = [
            "error-solutions",
            "best-practices",
            "codebase-context",
            "skills-patterns"
        ]

        for collection in collections:
            try:
                # Quick check: does collection have any content?
                info = self.qdrant.get_collection(collection)
                if info.points_count > 0:
                    available[collection] = info.points_count
            except Exception:
                continue

        return available

    async def identify_helpful_collections(
        self,
        gaps: List[str],
        response: str
    ) -> List[str]:
        """
        Identify which collections would be most helpful for addressing gaps

        Returns:
            List of collection names ordered by relevance
        """
        # Keyword-based heuristics
        helpful = set()

        gap_text = " ".join(gaps).lower()
        response_text = response.lower()
        combined = gap_text + " " + response_text

        # Error-related
        if any(word in combined for word in ["error", "fail", "issue", "problem", "broken"]):
            helpful.add("error-solutions")

        # Best practices
        if any(word in combined for word in ["best", "practice", "should", "recommend", "optimize"]):
            helpful.add("best-practices")

        # Code/configuration
        if any(word in combined for word in ["config", "code", "example", "file", "service"]):
            helpful.add("codebase-context")

        # Patterns/skills
        if any(word in combined for word in ["pattern", "workflow", "how to", "guide"]):
            helpful.add("skills-patterns")

        # Default: all collections if no specific hints
        if not helpful:
            helpful = {"error-solutions", "best-practices", "codebase-context"}

        return list(helpful)

    async def record_feedback(
        self,
        session_id: str,
        confidence: float,
        gaps: List[str],
        suggested_queries: List[str],
        metadata: Optional[Dict[str, Any]]
    ):
        """
        Record feedback for continuous learning

        Stores in interaction-history collection for future analysis
        """
        try:
            # Create feedback record
            feedback_record = {
                "session_id": session_id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "confidence": confidence,
                "gaps_count": len(gaps),
                "gaps": gaps,
                "suggested_queries": suggested_queries,
                "should_refine": confidence < self.confidence_threshold,
                "metadata": metadata or {}
            }

            embedding = await self.multi_turn_manager.embed_text(
                f"{session_id} {' '.join(gaps)} {' '.join(suggested_queries)}"
            )
            if not embedding:
                logger.warning("feedback_embedding_unavailable", session_id=session_id)
                return

            from qdrant_client.models import PointStruct

            point_id = str(uuid4())
            self.qdrant.upsert(
                collection_name="interaction-history",
                points=[
                    PointStruct(
                        id=point_id,
                        vector=embedding,
                        payload={
                            **feedback_record,
                            "type": "remote_llm_feedback",
                        },
                    )
                ],
            )

            logger.info(
                "feedback_recorded",
                session_id=session_id,
                point_id=point_id,
                confidence=confidence,
            )

        except Exception as e:
            logger.error(f"Error recording feedback: {e}")
