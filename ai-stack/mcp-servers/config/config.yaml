server:
  name: aidb-mcp
  host: 0.0.0.0
  port: 8791
  api_port: 8091
  workers: 1

database:
  postgres:
    host: postgres
    port: 5432
    database: mcp
    user: mcp
    password_file: /run/secrets/postgres_password  # Day 5: Use Docker secrets
    pool:
      size: 20  # Increased from 5 to handle concurrent agent requests
      max_overflow: 30  # Increased from 10 for burst capacity (total: 50 connections)
      timeout: 30
      recycle: 1800
      pre_ping: true
      use_lifo: true
  redis:
    host: redis
    port: 6379
    db: 0
    password_file: /run/secrets/redis_password  # Day 5: Use Docker secrets
    pool:
      max_connections: 50
      socket_timeout: 5
      socket_connect_timeout: 5

llm:
  llama_cpp:
    host: http://llama-cpp:8080
    models: []
  parallel_processing:
    enabled: false
    simple_model: GLM-4.5-Air-UD-Q4K-XL-GGUF
    complex_model: gpt-oss-20b-mxfp4-GGUF
    diversity_mode: false

tools:
  disclosure:
    full_requires_api_key: true
  sandbox:
    enabled: false
    runner: bubblewrap
    timeout: 30
    extra_args: []
  cache:
    ttl: 3600
    enabled: true
    embeddings_ttl: 86400
    vector_search_ttl: 300
  schema_cache: .mcp_cache/tool_schemas.json
  discovery_mode: minimal

logging:
  level: INFO
  file: /data/logs/aidb-mcp.log
  max_size: 10MB
  backup_count: 5

security:
  api_key: ""
  api_key_file: /run/secrets/stack_api_key
  rate_limit:
    enabled: true  # P1-SEC-002: Enable rate limiting for production hardening
    requests_per_minute: 60

rate_limiting:
  enabled: true
  requests_per_minute: 60

continuous_learning:
  checkpoint_interval: 100  # Events between checkpoints
  backpressure_threshold_mb: 100  # MB before pausing

rag:
  embedding_model: nomic-ai/nomic-embed-text-v1.5
  embedding_dimension: 768
  pgvector:
    hnsw_m: 16
    hnsw_ef_construction: 64

websearch:
  google_api_key: ""
  google_cse_id: ""

telemetry:
  enabled: true
  path: /data/telemetry/aidb-events.jsonl

embeddings:
  model: nomic-ai/nomic-embed-text-v1.5
  port: 8081
  max_input_length: 10000
  max_batch_size: 32
  request_timeout: 30
  model_load_retries: 3
  model_load_retry_delay: 5
  batch:
    max_size: 64
    max_latency_ms: 25
    queue_max: 1024

hybrid:
  server_port: 8092
  qdrant_url: http://qdrant:6333
  qdrant_hnsw:
    m: 16
    ef_construct: 64
    full_scan_threshold: 10000
  llama_cpp_url: http://llama-cpp:8080
  embedding_service_url: http://embeddings:8081
  embedding_model: nomic-ai/nomic-embed-text-v1.5
  embedding_dimensions: 768
  local_confidence_threshold: 0.7
  high_value_threshold: 0.7
  pattern_extraction_enabled: true

# Garbage Collection Configuration
garbage_collection:
  enabled: true
  schedule: "0 2 * * *"  # Daily at 2 AM
  max_solutions: 100000
  max_age_days: 30
  min_value_score: 0.5
  deduplicate_similarity: 0.95
  orphan_cleanup_enabled: true

# Query Validation Configuration
query_validation:
  enabled: true
  max_query_size: 10000  # 10KB
  max_results: 100
  max_offset: 10000
  rate_limit:
    requests_per_minute: 60
    requests_per_hour: 1000
  allowed_collections:
    - nixos_docs
    - solved_issues
    - skill_embeddings
    - telemetry_patterns
    - system_registry
    - tool_schemas
