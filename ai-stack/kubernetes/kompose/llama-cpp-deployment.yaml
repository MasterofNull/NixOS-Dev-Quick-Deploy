apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: /nix/store/x4z1n3wj7pscmifwnh2yvczxqg6vay39-kompose-1.37.0/bin/kompose convert -f ai-stack/compose/docker-compose.yml -o ai-stack/kubernetes/kompose
    kompose.version: 1.37.0 (HEAD)
    nixos.quick-deploy.ai-stack: "true"
    nixos.quick-deploy.service: llama_cpp
    nixos.quick-deploy.vulkan-enabled: "true"
  labels:
    io.kompose.service: llama-cpp
  name: llama-cpp
spec:
  replicas: 0
  selector:
    matchLabels:
      io.kompose.service: llama-cpp
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        kompose.cmd: /nix/store/x4z1n3wj7pscmifwnh2yvczxqg6vay39-kompose-1.37.0/bin/kompose convert -f ai-stack/compose/docker-compose.yml -o ai-stack/kubernetes/kompose
        kompose.version: 1.37.0 (HEAD)
        nixos.quick-deploy.ai-stack: "true"
        nixos.quick-deploy.service: llama_cpp
        nixos.quick-deploy.vulkan-enabled: "true"
      labels:
        io.kompose.service: llama-cpp
    spec:
      containers:
        - args:
            - --model
            - /models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
            - --host
            - 0.0.0.0
            - --port
            - "8080"
            - --ctx-size
            - "8192"
            - --threads
            - "0"
            - --batch-size
            - "512"
            - --ubatch-size
            - "128"
            - --defrag-thold
            - "0.1"
            - --cont-batching
            - --parallel
            - "4"
            - --cache-type-k
            - q4_0
            - --cache-type-v
            - q4_0
            - --metrics
            - --embeddings
          env:
            - name: LLAMA_VULKAN_ENABLE
            - name: VK_ICD_FILENAMES
              value: /usr/share/vulkan/icd.d/radeon_icd.x86_64.json
          envFrom:
            - configMapRef:
                name: env
          image: ghcr.io/ggml-org/llama.cpp:server
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - curl -sf http://localhost:8080/health | grep -q '"status":"ok"' || exit 1
            failureThreshold: 5
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
          name: local-ai-llama-cpp
          ports:
            - containerPort: 8080
              protocol: TCP
          volumeMounts:
            - mountPath: /dev/dri
              name: dri-device
          resources:
            limits:
              cpu: "4"
              memory: "17179869184"
            requests:
              cpu: "1"
              memory: "6442450944"
      restartPolicy: Always
      volumes:
        - name: dri-device
          hostPath:
            path: /dev/dri
            type: Directory
