version: "3.9"

services:
  postgres:
    image: timescale/timescaledb:latest-pg18
    container_name: local-ai-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${AIDB_POSTGRES_DB:-mcp}
      POSTGRES_USER: ${AIDB_POSTGRES_USER:-mcp}
      POSTGRES_PASSWORD: ${AIDB_POSTGRES_PASSWORD:-change_me}
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=C"
    volumes:
      - ${AI_STACK_DATA}/postgres:/var/lib/postgresql/data:Z
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIDB_POSTGRES_USER:-mcp}"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: local-ai-redis
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
      --requirepass ${AIDB_REDIS_PASSWORD:-}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
    volumes:
      - ${AI_STACK_DATA}/redis:/data:Z
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${AIDB_REDIS_PASSWORD:-}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  redis-insight:
    image: redis/redisinsight:latest
    container_name: local-ai-redisinsight
    restart: unless-stopped
    environment:
      - REDIS_HOSTS=local:${AIDB_REDIS_HOST:-redis}:${AIDB_REDIS_PORT:-6379}:${AIDB_REDIS_PASSWORD:-}
    ports:
      - "5540:5540"
    volumes:
      - ${AI_STACK_DATA}/redisinsight:/data:Z
    depends_on:
      - redis

  llama-cpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: local-ai-llama-cpp
    restart: unless-stopped
    command: >
      --model /models/${LLAMA_CPP_MODEL_FILE:-qwen2.5-coder-7b-instruct-q4_k_m.gguf}
      --host 0.0.0.0
      --port 8080
      --ctx-size 4096
      --n-gpu-layers 0
      --threads 4
    environment:
      LLAMA_CPP_HOST: 0.0.0.0
      LLAMA_CPP_PORT: ${LLAMA_CPP_PORT:-8080}
      LLAMA_CPP_DEFAULT_MODEL: ${LLAMA_CPP_DEFAULT_MODEL:-Qwen/Qwen2.5-Coder-7B-Instruct}
      LLAMA_CPP_LOG_LEVEL: ${LLAMA_CPP_LOG_LEVEL:-info}
      LLAMA_CPP_WEB_CONCURRENCY: ${LLAMA_CPP_WEB_CONCURRENCY:-4}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ${AI_STACK_DATA}/llama-cpp-models:/home/ubuntu/.cache/huggingface:Z
    ports:
      - "8080:8080"
    networks:
      default: {}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: local-ai-open-webui
    restart: unless-stopped
    ports:
      - "3001:8080"
    environment:
      OPENAI_API_BASE_URL: http://llama-cpp:8080/v1
      OPENAI_API_KEY: llama-cpp
      WEBUI_AUTH: "false"
      ENABLE_RAG_WEB_SEARCH: "true"
      ENABLE_OLLAMA_API: "false"
    volumes:
      - ${AI_STACK_DATA}/open-webui:/app/backend/data:Z
    depends_on:
      - llama-cpp

  qdrant:
    image: qdrant/qdrant:v1.16.2
    container_name: local-ai-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ${AI_STACK_DATA}/qdrant:/qdrant/storage:Z
    healthcheck:
      test: ["CMD", "true"]
      interval: 30s
      timeout: 5s
      retries: 3

  aidb:
    build:
      context: ./mcp-servers
      dockerfile: aidb/Dockerfile
    container_name: local-ai-aidb
    restart: unless-stopped
    ports:
      - "8091:8091"
    environment:
      AIDB_CONFIG: /app/config/config.yaml
      AIDB_POSTGRES_HOST: postgres
      AIDB_POSTGRES_PORT: 5432
      AIDB_POSTGRES_DB: ${AIDB_POSTGRES_DB:-mcp}
      AIDB_POSTGRES_USER: ${AIDB_POSTGRES_USER:-mcp}
      AIDB_POSTGRES_PASSWORD: ${AIDB_POSTGRES_PASSWORD:-change_me}
      AIDB_REDIS_HOST: ${AIDB_REDIS_HOST:-redis}
      AIDB_REDIS_PORT: ${AIDB_REDIS_PORT:-6379}
      AIDB_REDIS_PASSWORD: ${AIDB_REDIS_PASSWORD:-}
      LLAMA_CPP_BASE_URL: http://llama-cpp:8080
      AIDB_TELEMETRY_PATH: /data/telemetry/aidb-events.jsonl
    volumes:
      - ${AI_STACK_DATA}/aidb:/data:Z
      - ${AI_STACK_DATA}/telemetry:/data/telemetry:Z
      - ${AI_STACK_DATA}/aidb-cache:/app/.mcp_cache:Z
    depends_on:
      - postgres
      - redis
      - llama-cpp
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8091/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  hybrid-coordinator:
    build:
      context: ./mcp-servers
      dockerfile: hybrid-coordinator/Dockerfile
    container_name: local-ai-hybrid-coordinator
    restart: unless-stopped
    ports:
      - "8092:8092"
    environment:
      MCP_SERVER_MODE: http
      MCP_SERVER_PORT: ${MCP_SERVER_PORT:-8092}
      QDRANT_URL: http://qdrant:6333
      LLAMA_CPP_BASE_URL: http://llama-cpp:8080
      LOCAL_CONFIDENCE_THRESHOLD: ${LOCAL_CONFIDENCE_THRESHOLD:-0.7}
      HIGH_VALUE_THRESHOLD: ${HIGH_VALUE_THRESHOLD:-0.7}
      PATTERN_EXTRACTION_ENABLED: ${PATTERN_EXTRACTION_ENABLED:-true}
      FINETUNE_DATA_PATH: /data/fine-tuning/dataset.jsonl
      HYBRID_TELEMETRY_PATH: /data/telemetry/hybrid-events.jsonl
      TELEMETRY_PATH: /data/telemetry/hybrid-events.jsonl
    volumes:
      - ${AI_STACK_DATA}/hybrid-coordinator:/data:Z,U
      - ${AI_STACK_DATA}/telemetry:/data/telemetry:Z,U
      - ${AI_STACK_DATA}/fine-tuning:/data/fine-tuning:Z,U
    depends_on:
      - qdrant
      - llama-cpp
      - aidb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8092/health"]
      interval: 30s
      timeout: 10s
      retries: 5

networks:
  default:
    name: local-ai-trimmed
