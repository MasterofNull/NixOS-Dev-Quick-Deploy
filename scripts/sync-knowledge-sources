#!/usr/bin/env bash
# sync-knowledge-sources — import all enabled knowledge sources into AIDB.
#
# Reads ai-stack/data/knowledge-sources.yaml and, for each enabled source,
# fetches content and pipes it to the AIDB import endpoint.
#
# Usage:
#   sync-knowledge-sources              # sync all enabled sources
#   sync-knowledge-sources --id claude-code-templates   # sync one source
#   sync-knowledge-sources --dry-run    # show what would be synced
#   sync-knowledge-sources --list       # list all registered sources

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
SOURCES_FILE="$REPO_ROOT/ai-stack/data/knowledge-sources.yaml"
AIDB_URL="${AIDB_URL:-http://127.0.0.1:${AI_AIDB_PORT:-${AIDB_PORT:-8002}}}"
AIDB_IMPORT_ENDPOINT="${AIDB_IMPORT_ENDPOINT:-/documents}"
AIDB_KEY_FILE="${AIDB_API_KEY_FILE:-/run/secrets/aidb_api_key}"
CACHE_DIR="${XDG_CACHE_HOME:-$HOME/.cache}/nixos-ai-stack/knowledge-sources"

DRY_RUN=false
FILTER_ID=""
LIST_ONLY=false

while [[ $# -gt 0 ]]; do
    case "$1" in
        --dry-run)   DRY_RUN=true; shift ;;
        --id)        FILTER_ID="$2"; shift 2 ;;
        --list)      LIST_ONLY=true; shift ;;
        -h|--help)
            echo "Usage: sync-knowledge-sources [--id ID] [--dry-run] [--list]"
            exit 0 ;;
        *) echo "Unknown option: $1"; exit 1 ;;
    esac
done

# Read AIDB API key
AIDB_KEY=""
if [[ -r "$AIDB_KEY_FILE" ]]; then
    AIDB_KEY=$(tr -d '[:space:]' < "$AIDB_KEY_FILE")
else
    AIDB_KEY="${AIDB_API_KEY:-}"
fi

if ! command -v python3 &>/dev/null; then
    echo "Error: python3 required" >&2; exit 1
fi

if [[ "$LIST_ONLY" == "true" ]]; then
    echo "=== Registered Knowledge Sources ==="
    python3 - "$SOURCES_FILE" <<'PYEOF'
import sys, yaml
with open(sys.argv[1]) as f:
    config = yaml.safe_load(f)
for s in config.get("sources", []):
    status = "enabled" if s.get("enabled", True) else "disabled"
    print(f"  [{status:8s}] {s['id']:40s} {s['type']:12s} {s['description'][:60]}...")
PYEOF
    exit 0
fi

mkdir -p "$CACHE_DIR"

# Parse YAML and process sources
python3 - "$SOURCES_FILE" "$FILTER_ID" "$DRY_RUN" "$AIDB_URL" "$AIDB_IMPORT_ENDPOINT" "$AIDB_KEY" "$CACHE_DIR" <<'PYEOF'
import sys, json, os, hashlib, tempfile, subprocess, time
from pathlib import Path
from datetime import datetime, timezone

try:
    import yaml
except ImportError:
    print("Error: pyyaml required (pip install pyyaml)", file=sys.stderr)
    sys.exit(1)

sources_file, filter_id, dry_run_str, aidb_url, aidb_import_endpoint, aidb_key, cache_dir = sys.argv[1:8]
dry_run = dry_run_str == "True"
cache_dir = Path(cache_dir)
max_doc_bytes = int(os.getenv("AIDB_IMPORT_MAX_BYTES", "200000"))
import_delay_ms = int(os.getenv("AIDB_IMPORT_DELAY_MS", "75"))
max_retries = int(os.getenv("AIDB_IMPORT_MAX_RETRIES", "4"))

with open(sources_file) as f:
    config = yaml.safe_load(f)

import urllib.request, urllib.error

def gh_raw(owner_repo, path):
    """Fetch a file from GitHub raw content."""
    url = f"https://raw.githubusercontent.com/{owner_repo}/main/{path}"
    try:
        with urllib.request.urlopen(url, timeout=30) as resp:
            return resp.read().decode("utf-8")
    except urllib.error.HTTPError as e:
        print(f"  WARN: {url} → HTTP {e.code}", file=sys.stderr)
        return None

def split_content(text, max_bytes):
    """Split text into UTF-8-safe chunks that fit within max_bytes (best effort)."""
    if len(text.encode("utf-8")) <= max_bytes:
        return [text]
    data = text.encode("utf-8")
    chunks = []
    for i in range(0, len(data), max_bytes):
        chunks.append(data[i:i + max_bytes].decode("utf-8", errors="ignore"))
    return [c for c in chunks if c]


def aidb_import(collection, content, source_id, aidb_url, endpoint, aidb_key):
    """POST content to AIDB import endpoint (default: /documents)."""
    import urllib.request, urllib.error, json as _json
    payload = _json.dumps({
        "project": collection,
        "relative_path": source_id,
        "title": source_id.rsplit("/", 1)[-1],
        "content_type": "text/markdown",
        "checksum": hashlib.sha256(content.encode("utf-8")).hexdigest(),
        "content": content,
        "status": "approved",
    }).encode("utf-8")
    headers = {"Content-Type": "application/json"}
    if aidb_key:
        headers["X-API-Key"] = aidb_key
    for attempt in range(max_retries + 1):
        req = urllib.request.Request(f"{aidb_url}{endpoint}", data=payload, headers=headers, method="POST")
        try:
            with urllib.request.urlopen(req, timeout=60) as resp:
                result = _json.loads(resp.read())
                if import_delay_ms > 0:
                    time.sleep(import_delay_ms / 1000.0)
                return result
        except urllib.error.HTTPError as e:
            retriable = e.code in (429, 500, 502, 503, 504)
            if retriable and attempt < max_retries:
                backoff = min(2 ** attempt, 10)
                print(f"  WARN: import retry ({e.code}) source={source_id} attempt={attempt+1}/{max_retries} backoff={backoff}s", file=sys.stderr)
                time.sleep(backoff)
                continue
            print(f"  ERROR importing to AIDB: HTTP {e.code} source={source_id}", file=sys.stderr)
            return None
        except Exception as e:
            if attempt < max_retries:
                backoff = min(2 ** attempt, 10)
                print(f"  WARN: import retry source={source_id} attempt={attempt+1}/{max_retries} backoff={backoff}s err={e}", file=sys.stderr)
                time.sleep(backoff)
                continue
            print(f"  ERROR importing to AIDB: {e}", file=sys.stderr)
            return None

synced = 0
skipped = 0

for source in config.get("sources", []):
    sid = source["id"]
    if filter_id and sid != filter_id:
        continue
    if not source.get("enabled", True):
        print(f"[skip] {sid} (disabled)")
        skipped += 1
        continue

    print(f"\n[sync] {sid} ({source['type']})")
    collection = source.get("collection", sid)
    stype = source["type"]
    fetch_specs = source.get("fetch", ["readme"])
    if isinstance(fetch_specs, str):
        fetch_specs = [fetch_specs]

    if dry_run:
        print(f"  DRY RUN: would fetch {fetch_specs} → collection={collection}")
        continue

    texts = []

    if stype == "github_repo":
        owner_repo = source["url"]
        for spec in fetch_specs:
            if spec == "readme":
                content = gh_raw(owner_repo, "README.md")
                if content:
                    texts.append(("README.md", content))
            elif spec.startswith("file:"):
                path = spec[5:]
                content = gh_raw(owner_repo, path)
                if content:
                    texts.append((path, content))

    elif stype == "url":
        import urllib.request as _ureq
        try:
            with _ureq.urlopen(source["url"], timeout=30) as resp:
                texts.append((source["url"], resp.read().decode("utf-8")))
        except Exception as e:
            print(f"  ERROR fetching {source['url']}: {e}", file=sys.stderr)

    elif stype == "local_dir":
        local = Path(source["url"])
        if not local.is_absolute():
            local = Path(sources_file).parents[2] / local
        for p in sorted(local.rglob("*.md")):
            texts.append((str(p.relative_to(local)), p.read_text()))

    for name, text in texts:
        parts = split_content(text, max_doc_bytes)
        for idx, part in enumerate(parts, start=1):
            source_id = f"{sid}/{name}" if len(parts) == 1 else f"{sid}/{name}#part-{idx}"
            print(f"  → importing {source_id} ({len(part):,} chars) into {collection}")
            result = aidb_import(collection, part, source_id, aidb_url, aidb_import_endpoint, aidb_key)
            if result:
                print(f"    OK: {result.get('status', 'ok')}")
                synced += 1
            else:
                skipped += 1

print(f"\n=== Done: {synced} imports, {skipped} skipped ===")
PYEOF
