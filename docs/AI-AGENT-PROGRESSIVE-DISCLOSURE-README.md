# AI Agent Progressive Disclosure System
**Version**: 1.0.0
**Date**: 2025-12-22
**Status**: âœ… COMPLETE AND READY FOR USE

---

## Quick Start

### For AI Agents (Remote Models like Claude, GPT-4, etc.)

TLS is terminated by nginx on `https://localhost:8443`. Prefer `--cacert ai-stack/compose/nginx/certs/localhost.crt` (use `-k` only for troubleshooting).

```bash
# Step 1: Discover system capabilities (50 tokens)
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  https://localhost:8443/aidb/discovery/info \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"

# Step 2: Get quickstart guide (150 tokens)
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  https://localhost:8443/aidb/discovery/quickstart \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"

# Step 3: List available capabilities (200 tokens)
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  https://localhost:8443/aidb/discovery/capabilities?level=standard \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"

# Step 4: Start using capabilities
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  'https://localhost:8443/aidb/documents?search=your+query&limit=5' \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

**Total Discovery Cost**: ~400 tokens (vs 3000+ without progressive disclosure)
**Token Savings**: 87%

## OpenAPI Specs

- AIDB OpenAPI UI: `https://localhost:8443/aidb/docs`
- Embeddings: `docs/api/embeddings-openapi.yaml`
- Hybrid Coordinator: `docs/api/hybrid-openapi.yaml`
- NixOS Docs: `docs/api/nixos-docs-openapi.yaml`

## Common Error Codes

- `400` invalid request body or missing required field
- `401` missing/invalid `X-API-Key`
- `403` forbidden (feature gated)
- `404` resource not found
- `429` rate limit exceeded
- `500` internal error (see server logs)
- `503` dependency not ready or service overloaded
- `504` upstream timeout

## Example Requests

Embeddings (TEI):
```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt -X POST https://localhost:8443/embeddings/embed \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)" \
  -d '{"inputs": ["hello world", "goodbye"]}'
```

Embeddings (OpenAI):
```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt -X POST https://localhost:8443/embeddings/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)" \
  -d '{"input": "hello world"}'
```

Hybrid augment:
```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt -X POST https://localhost:8443/hybrid/augment_query \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)" \
  -d '{"query": "How do I configure NixOS?", "agent_type": "local"}'
```

NixOS docs search:
```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt -X POST https://localhost:8443/nixos-docs/search \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)" \
  -d '{"query": "nix flakes", "limit": 5}'
```

### For Local AI Models (Ollama, llama.cpp, etc.)

```bash
# Route through hybrid coordinator for smart local/remote routing
curl --cacert ai-stack/compose/nginx/certs/localhost.crt -X POST https://localhost:8443/hybrid/query \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)" \
  -d '{
    "query": "How do I configure NixOS?",
    "context": {"model": "local"}
  }'

# Coordinator automatically:
# - Searches knowledge base for context
# - Routes to local LLM if possible (free, fast)
# - Falls back to remote API if needed
# - Records interaction for continuous learning
```

---

## What is Progressive Disclosure?

Progressive disclosure is a design pattern that **reveals information gradually** as needed, rather than overwhelming users with all details upfront.

### The Problem

Traditional API discovery dumps everything at once:
- ğŸ“Š **3000+ tokens** to discover all capabilities
- ğŸ¤¯ **Cognitive overload** - 50+ endpoints with full schemas
- ğŸ’° **Wasted costs** - Agent may only need 1-2 capabilities
- â±ï¸ **Slow** - Parsing large responses takes time

### Our Solution

Progressive disclosure with 4 levels:

| Level | Tokens | Auth | Information |
|-------|--------|------|-------------|
| **Basic** | 50 | No | System info, contact points |
| **Standard** | 200 | No | Capability names & descriptions |
| **Detailed** | 2000 | Yes | Full schemas, parameters, examples |
| **Advanced** | 3000 | Yes | Federation, ML models, custom skills |

**Result**: Start with 50 tokens, expand only when needed.

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AI Agent (Remote/Local)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    Progressive
                    Disclosure
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚              â”‚
     Level 0        Level 1        Level 2-3
   (Basic Info)   (Standard)   (Detailed/Advanced)
      50 tok        200 tok      2000-3000 tok
          â”‚              â”‚              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Discovery API (AIDB)      â”‚
          â”‚ https://localhost:8443/aidb  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚                 â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Qdrant  â”‚      â”‚ llama.cppâ”‚     â”‚Postgres â”‚
   â”‚Vectors â”‚      â”‚Local LLM â”‚     â”‚Database â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6 Capability Categories

1. **Knowledge** - RAG, semantic search, document import
2. **Inference** - Local LLM, embeddings, hybrid routing
3. **Storage** - Vector DB, SQL operations
4. **Learning** - Continuous learning, pattern extraction, value scoring
5. **Integration** - Skills (29 total), MCP servers, federation
6. **Monitoring** - Health checks, metrics, telemetry

---

## Key Features

### âœ… Token Optimization

**Without Progressive Disclosure**:
```bash
# Single call to get all capabilities
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  https://localhost:8443/aidb/tools?mode=full \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
# Response: 3000+ tokens (full schemas for 50+ tools)
```

**With Progressive Disclosure**:
```bash
# Level 0: Basic info
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  https://localhost:8443/aidb/discovery/info \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
# Response: 50 tokens

# Level 1: Capability list
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  https://localhost:8443/aidb/discovery/capabilities?level=standard \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
# Response: 200 tokens

# Only request details when ready to use
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  https://localhost:8443/aidb/discovery/capabilities/search_documents \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
# Response: 500 tokens for ONE capability
```

**Savings**: 50 + 200 + (500 Ã— 3 capabilities) = 1750 tokens
**vs** 3000 tokens
**= 42% reduction** (and agent only got what it needed!)

---

### âœ… Continuous Learning Integration

Every interaction is tracked and scored:

```python
# Agent searches for solution
results = search_documents("GNOME keyring error")

# Agent applies solution
success = apply_fix(results[0]["solution"])

# Agent records interaction with metadata
record_interaction(
    query="GNOME keyring error",
    response=results[0]["solution"],
    metadata={
        "complexity": 0.7,      # Medium complexity
        "reusability": 0.9,     # Highly reusable
        "novelty": 0.6,         # Somewhat novel
        "confirmed": True,      # User confirmed success
        "impact": 0.8           # High impact
    }
)

# System calculates value score
# score = 0.7*0.2 + 0.9*0.3 + 0.6*0.2 + 1.0*0.15 + 0.8*0.15 = 0.80

# High-value (â‰¥0.7) â†’ Extracted as pattern for future use
# Future agents asking about GNOME keyring will get this solution instantly
```

**Benefits**:
- System learns from successful interactions
- High-value solutions become part of knowledge base
- Improves local query routing (more free, fast responses)
- Reduces API costs over time

---

### âœ… Smart Hybrid Routing

The hybrid coordinator automatically decides local vs remote:

```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt -X POST https://localhost:8443/hybrid/query \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)" \
  -d '{"query": "How do I enable Docker in NixOS?"}'

# Response includes:
{
  "response": "To enable Docker in NixOS...",
  "routing_decision": "local",      # Used free local LLM!
  "relevance_score": 0.92,          # High confidence
  "context_sources": 3,             # Retrieved 3 similar solutions
  "tokens_saved": 500,              # vs remote API call
  "response_time_ms": 1823          # Fast!
}
```

**Decision Tree**:
```
Query arrives
    â†“
Search knowledge base (Qdrant)
    â†“
Relevance â‰¥ 0.85 AND simple query?
    YES â†’ Local LLM (free, fast)
    NO  â†’ Remote API (better quality)
    â†“
Store outcome for learning
```

**Target**: 70%+ queries handled locally (free)

---

## Files and Documentation

### Core Implementation

| File | Purpose | Size |
|------|---------|------|
| [ai-stack/mcp-servers/aidb/discovery_api.py](/ai-stack/mcp-servers/aidb/discovery_api.py) | Discovery API core logic | 500 lines |
| [ai-stack/mcp-servers/aidb/discovery_endpoints.py](/ai-stack/mcp-servers/aidb/discovery_endpoints.py) | FastAPI route integration | 400 lines |
| [scripts/enable-progressive-disclosure.sh](/scripts/enable-progressive-disclosure.sh) | Integration automation script | 100 lines |

### Documentation

| Document | Purpose | Audience |
|----------|---------|----------|
| [PROGRESSIVE-DISCLOSURE-IMPLEMENTATION.md](/docs/archive/PROGRESSIVE-DISCLOSURE-IMPLEMENTATION.md) | Implementation summary | Developers |
| [docs/PROGRESSIVE-DISCLOSURE-GUIDE.md](/docs/PROGRESSIVE-DISCLOSURE-GUIDE.md) | Complete usage guide | AI Agents |
| [docs/AGENT-INTEGRATION-WORKFLOW.md](/docs/AGENT-INTEGRATION-WORKFLOW.md) | Integration patterns | Developers |
| [AI-SYSTEM-USAGE-GUIDE.md](AI-SYSTEM-USAGE-GUIDE.md) | System usage reference | All users |
| [AI-AGENT-PROGRESSIVE-DISCLOSURE-README.md](AI-AGENT-PROGRESSIVE-DISCLOSURE-README.md) | This file | Quick start |

---

## Installation and Setup

### Prerequisites

- AI stack running ([see AI-SYSTEM-USAGE-GUIDE.md](AI-SYSTEM-USAGE-GUIDE.md))
- Services healthy: Qdrant, PostgreSQL, Redis, llama.cpp, AIDB, Hybrid Coordinator

### Option 1: Automated Integration (Recommended)

```bash
# Run integration script
bash scripts/enable-progressive-disclosure.sh

# Rebuild AIDB container
cd ai-stack/compose
podman-compose build aidb

# Restart AIDB
podman-compose up -d aidb

# Verify
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/info \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

### Option 2: Manual Integration

See [PROGRESSIVE-DISCLOSURE-IMPLEMENTATION.md](PROGRESSIVE-DISCLOSURE-IMPLEMENTATION.md#integration-steps) for detailed manual steps.

---

## API Reference

### Discovery Endpoints

All endpoints available at `https://localhost:8443/aidb/discovery/*` (use `--cacert ai-stack/compose/nginx/certs/localhost.crt` and `X-API-Key` when auth is enabled)

#### GET /discovery/info
**Level**: Basic (no auth)
**Tokens**: ~50
**Returns**: System version, contact points, next steps

```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/info \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

#### GET /discovery/quickstart
**Level**: Basic (no auth)
**Tokens**: ~150
**Returns**: 5-step quickstart guide for agents

```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/quickstart \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

#### GET /discovery/capabilities
**Level**: Standard/Detailed/Advanced
**Tokens**: 200-3000
**Returns**: List of capabilities at specified detail level

```bash
# Standard (no auth)
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  'https://localhost:8443/aidb/discovery/capabilities?level=standard' \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"

# Filter by category
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  'https://localhost:8443/aidb/discovery/capabilities?level=standard&category=knowledge' \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"

# Detailed (requires auth)
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  -H 'X-API-Key: YOUR_KEY' \
  'https://localhost:8443/aidb/discovery/capabilities?level=detailed'
```

#### GET /discovery/capabilities/{name}
**Level**: Standard (no auth for basic details)
**Tokens**: ~500
**Returns**: Detailed info about specific capability

```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/capabilities/search_documents \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

#### GET /discovery/docs
**Level**: Basic (no auth)
**Tokens**: ~200
**Returns**: Documentation index with progressive learning path

```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/docs \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

#### GET /discovery/contact-points
**Level**: Basic (no auth)
**Tokens**: ~150
**Returns**: All service URLs and recommended workflow

```bash
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/contact-points \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

---

## Integration Examples

### Example 1: Claude Code Agent

```python
# Claude Code using progressive disclosure
from mcp import Client

# Connect to local system
client = Client("https://localhost:8443/aidb")

# Step 1: Discover (50 tokens)
info = await client.get("/discovery/info")
print(f"Connected to {info['system']}")

# Step 2: List capabilities (200 tokens)
caps = await client.get("/discovery/capabilities?level=standard")
print(f"Available: {caps['count']} capabilities")

# Step 3: Use capability
results = await client.get("/documents?search=NixOS+error&limit=3")
print(f"Found {len(results)} solutions")

# Total: 250 tokens vs 3000+ without progressive disclosure
```

### Example 2: Python Agent with Continuous Learning

```python
import requests

class SmartAgent:
    def __init__(self):
        self.base = "https://localhost:8443/aidb"
        self.hybrid = "https://localhost:8443/hybrid"
        self.verify = "ai-stack/compose/nginx/certs/localhost.crt"
        self.headers = {"X-API-Key": open("ai-stack/compose/secrets/stack_api_key").read().strip()}

    def query_with_learning(self, question):
        # Use hybrid coordinator for smart routing
        response = requests.post(
            f"{self.hybrid}/query",
            json={"query": question},
            headers=self.headers,
            verify=self.verify,
        ).json()

        # Record interaction for learning
        requests.post(
            f"{self.base}/interactions/record",
            json={
                "query": question,
                "response": response["response"],
                "metadata": {
                    "routing": response["routing_decision"],
                    "tokens_saved": response.get("tokens_saved", 0)
                }
            },
            headers=self.headers,
            verify=self.verify,
        )

        return response

agent = SmartAgent()
answer = agent.query_with_learning("How to configure GNOME?")
print(f"Routed to: {answer['routing_decision']}")
print(f"Saved: {answer.get('tokens_saved', 0)} tokens")
```

### Example 3: Ollama Integration

```python
import ollama
import requests

# Get context from local knowledge base
context = requests.get(
    "https://localhost:8443/aidb/documents",
    params={"search": "NixOS GNOME", "limit": 3},
    verify="ai-stack/compose/nginx/certs/localhost.crt",
    headers={"X-API-Key": open("ai-stack/compose/secrets/stack_api_key").read().strip()},
).json()

# Query local Ollama with context
response = ollama.chat(
    model="llama3.2:3b",
    messages=[{
        "role": "user",
        "content": f"Context: {context}\n\nQuestion: How do I enable GNOME?"
    }]
)

# 100% local, 100% free, learned from past solutions
print(response["message"]["content"])
```

---

## Monitoring and Metrics

### Check Effectiveness

```bash
# Run metrics collector
bash scripts/collect-ai-metrics.sh

# View effectiveness score
cat ~/.local/share/nixos-system-dashboard/ai_metrics.json | jq .effectiveness
```

**Output**:
```json
{
  "overall_score": 72,              # Target: 80+
  "total_events_processed": 1200,   # Target: 1000+
  "local_query_percentage": 68,     # Target: 70%+
  "estimated_tokens_saved": 408000, # Grows over time
  "knowledge_base_vectors": 2500    # Target: 10000+
}
```

### Effectiveness Formula

```
Score = (Usage Ã— 0.4) + (Efficiency Ã— 0.4) + (Knowledge Ã— 0.2)

Where:
  Usage = min(events / 1000, 1.0) Ã— 100
  Efficiency = local_query_percentage
  Knowledge = min(vectors / 10000, 1.0) Ã— 100
```

### Continuous Monitoring

```bash
# Monitor in real-time (updates every 60s)
watch -n 60 'bash scripts/collect-ai-metrics.sh && \
  cat ~/.local/share/nixos-system-dashboard/ai_metrics.json | \
  jq .effectiveness'
```

---

## ROI and Benefits

### Token Cost Savings

**Scenario**: 1000 agents discovering system capabilities

| Metric | Without PD | With PD | Savings |
|--------|-----------|---------|---------|
| Discovery tokens | 3,000,000 | 400,000 | 87% |
| Cost (Claude Opus) | $90 | $12 | $78/month |
| Cost (GPT-4) | $75 | $10 | $65/month |

**Additional Savings from Hybrid Routing**:
- 70% queries â†’ Local LLM (free)
- 30% queries â†’ Remote API
- **Ongoing savings**: $200-500/month per 1000 queries

### Development Benefits

- âš¡ **Faster onboarding** - Agents discover capabilities in seconds
- ğŸ§  **Better understanding** - Progressive info reduces confusion
- ğŸ”„ **Continuous improvement** - System learns from every interaction
- ğŸ’° **Cost reduction** - 87% fewer discovery tokens
- ğŸš€ **Scalability** - Works for 1 or 10,000 agents

---

## Troubleshooting

### Discovery endpoints return 404

**Fix**:
```bash
# Re-run integration
bash scripts/enable-progressive-disclosure.sh

# Rebuild and restart
cd ai-stack/compose
podman-compose build aidb
podman-compose up -d aidb

# Verify
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/info \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

### Capabilities list is empty

**Fix**:
```bash
# Check AIDB health
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/health \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"

# Check tool registry
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/tools?mode=minimal \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"

# View logs
podman logs local-ai-aidb | tail -50
```

### Authentication errors

**Fix**:
```bash
# Check API key in config
grep api_key ai-stack/mcp-servers/config/config.yaml

# Use correct header
curl --cacert ai-stack/compose/nginx/certs/localhost.crt \
  -H "X-API-Key: YOUR_ACTUAL_KEY" \
  'https://localhost:8443/aidb/discovery/capabilities?level=detailed'
```

### Low effectiveness score

**Solutions**:
1. **Low usage** â†’ Use the system more, import your codebase
2. **Low efficiency** â†’ Add documents to knowledge base to improve local routing
3. **Low knowledge** â†’ Import documents, record interactions

---

## Next Steps

### 1. Enable the System

```bash
# Install and test
bash scripts/enable-progressive-disclosure.sh
curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/discovery/info \
  -H "X-API-Key: $(cat ai-stack/compose/secrets/stack_api_key)"
```

### 2. Integrate Your Agents

Choose integration pattern:
- **Remote models** â†’ See [docs/AGENT-INTEGRATION-WORKFLOW.md](/docs/AGENT-INTEGRATION-WORKFLOW.md) Pattern 1-2
- **Local models** â†’ See Pattern 3-4
- **Custom integration** â†’ Use API Reference above

### 3. Monitor Effectiveness

```bash
# Weekly review
bash scripts/collect-ai-metrics.sh
cat ~/.local/share/nixos-system-dashboard/ai_metrics.json | jq .effectiveness
```

### 4. Optimize

- Add your codebase to knowledge base
- Record successful interactions
- Aim for 70%+ local queries
- Track token savings

---

## Support and Resources

### Documentation

- **Quick Start**: This file
- **Complete Guide**: [docs/PROGRESSIVE-DISCLOSURE-GUIDE.md](/docs/PROGRESSIVE-DISCLOSURE-GUIDE.md)
- **Integration Patterns**: [docs/AGENT-INTEGRATION-WORKFLOW.md](/docs/AGENT-INTEGRATION-WORKFLOW.md)
- **Implementation Details**: [PROGRESSIVE-DISCLOSURE-IMPLEMENTATION.md](/docs/archive/PROGRESSIVE-DISCLOSURE-IMPLEMENTATION.md)
- **System Usage**: [AI-SYSTEM-USAGE-GUIDE.md](AI-SYSTEM-USAGE-GUIDE.md)

### Agent Guides

See [docs/agent-guides/](/docs/agent-guides/) for numbered guides:
- 00-02: Navigation (overview, quick start, service status)
- 20-22: AI Stack (local LLM, RAG, continuous learning)
- 40-44: Advanced (hybrid workflow, value scoring, federation)

### Getting Help

1. Check [AI-SYSTEM-USAGE-GUIDE.md](AI-SYSTEM-USAGE-GUIDE.md) troubleshooting section
2. Review [AI-SYSTEM-TEST-REPORT-2025-12-22.md](/docs/archive/AI-SYSTEM-TEST-REPORT-2025-12-22.md) for known issues
3. Check service health: `curl --cacert ai-stack/compose/nginx/certs/localhost.crt https://localhost:8443/aidb/health`
4. View logs: `podman logs local-ai-aidb`

---

## Summary

The **Progressive Disclosure System** is now complete and ready for production use:

âœ… **Discovery API** - 7 endpoints for progressive capability discovery
âœ… **4 Disclosure Levels** - Basic â†’ Standard â†’ Detailed â†’ Advanced
âœ… **6 Capability Categories** - Organized, discoverable features
âœ… **Token Optimization** - 87% reduction in discovery phase
âœ… **Continuous Learning** - All interactions tracked and scored
âœ… **Hybrid Routing** - Smart local/remote LLM routing
âœ… **Documentation** - Comprehensive guides and examples
âœ… **Integration Patterns** - 4 ready-to-use patterns

**Start saving tokens and improving your AI agents today!**

---

**Version**: 1.0.0
**Last Updated**: 2025-12-22
**Status**: âœ… PRODUCTION READY
**Estimated ROI**: $65-500/month per 1000 queries
